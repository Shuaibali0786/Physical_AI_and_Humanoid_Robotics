"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[296],{7689:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module4-vla/intro","title":"Module 4: Vision-Language-Action (VLA)","description":"Introduction","source":"@site/docs/module4-vla/intro.md","sourceDirName":"module4-vla","slug":"/module4-vla/intro","permalink":"/docs/module4-vla/intro","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"modules","previous":{"title":"Sim-to-Real Transfer","permalink":"/docs/module3-isaac/sim-to-real"},"next":{"title":"Voice-to-Action with OpenAI Whisper","permalink":"/docs/module4-vla/voice-to-action"}}');var t=o(4848),a=o(8453);const r={},s="Module 4: Vision-Language-Action (VLA)",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Module Overview",id:"module-overview",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Welcome to Module 4: Vision-Language-Action (VLA). This module explores the cutting-edge integration of vision, language, and action systems in humanoid robotics. VLA represents a paradigm shift toward more natural human-robot interaction, where robots can understand complex verbal commands, perceive their environment visually, and execute appropriate physical actions in response."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, you will understand:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"How to implement voice-to-action systems using OpenAI Whisper"}),"\n",(0,t.jsx)(n.li,{children:"Techniques for cognitive planning with LLMs to control ROS 2 actions"}),"\n",(0,t.jsx)(n.li,{children:"Methods for speech recognition and natural language understanding"}),"\n",(0,t.jsx)(n.li,{children:"Multi-modal interaction combining speech, gesture, and vision"}),"\n",(0,t.jsx)(n.li,{children:"How to develop a capstone project for autonomous humanoid operation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"module-overview",children:"Module Overview"}),"\n",(0,t.jsx)(n.p,{children:"Vision-Language-Action systems enable humanoid robots to interact with humans in more natural, intuitive ways. This module covers the technical foundations and practical implementation of VLA systems, combining state-of-the-art AI technologies with robotic control to create more capable and user-friendly robots."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>s});var i=o(6540);const t={},a=i.createContext(t);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);