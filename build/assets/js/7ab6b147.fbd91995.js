"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[11],{6563:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"module4-vla/multimodal-interaction","title":"Multi-modal Interaction: Speech, Gesture, and Vision","description":"Introduction to Multi-modal Interaction","source":"@site/docs/module4-vla/multimodal-interaction.md","sourceDirName":"module4-vla","slug":"/module4-vla/multimodal-interaction","permalink":"/docs/module4-vla/multimodal-interaction","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"modules","previous":{"title":"Speech Recognition and Natural Language Understanding","permalink":"/docs/module4-vla/speech-recognition"},"next":{"title":"Capstone Project: Autonomous Humanoid","permalink":"/docs/module4-vla/capstone-project"}}');var i=t(4848),o=t(8453);const s={},a="Multi-modal Interaction: Speech, Gesture, and Vision",c={},l=[{value:"Introduction to Multi-modal Interaction",id:"introduction-to-multi-modal-interaction",level:2},{value:"The Multi-modal Framework",id:"the-multi-modal-framework",level:2},{value:"Integration Architecture",id:"integration-architecture",level:3},{value:"Synchronization and Fusion",id:"synchronization-and-fusion",level:3},{value:"Speech Processing in Multi-modal Context",id:"speech-processing-in-multi-modal-context",level:2},{value:"Context-Aware Speech Recognition",id:"context-aware-speech-recognition",level:3},{value:"Gesture Recognition and Interpretation",id:"gesture-recognition-and-interpretation",level:2},{value:"Gesture Detection Pipeline",id:"gesture-detection-pipeline",level:3},{value:"3D Gesture Mapping",id:"3d-gesture-mapping",level:3},{value:"Vision Processing and Scene Understanding",id:"vision-processing-and-scene-understanding",level:2},{value:"Object Detection and Recognition",id:"object-detection-and-recognition",level:3},{value:"Multi-modal Fusion and Interpretation",id:"multi-modal-fusion-and-interpretation",level:2},{value:"Fusion Engine",id:"fusion-engine",level:3},{value:"Integration with Robot Control",id:"integration-with-robot-control",level:2},{value:"Action Execution System",id:"action-execution-system",level:3},{value:"Real-time Processing and Optimization",id:"real-time-processing-and-optimization",level:2},{value:"Efficient Multi-modal Pipeline",id:"efficient-multi-modal-pipeline",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"multi-modal-interaction-speech-gesture-and-vision",children:"Multi-modal Interaction: Speech, Gesture, and Vision"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction-to-multi-modal-interaction",children:"Introduction to Multi-modal Interaction"}),"\n",(0,i.jsx)(n.p,{children:"Multi-modal interaction in humanoid robotics combines multiple sensory channels\u2014speech, gesture, and vision\u2014to create more natural and intuitive human-robot communication. Unlike single-modal systems, multi-modal interaction allows humans to communicate with robots using the same rich, contextual communication methods they use with other humans, significantly improving the effectiveness and naturalness of human-robot interaction."}),"\n",(0,i.jsx)(n.h2,{id:"the-multi-modal-framework",children:"The Multi-modal Framework"}),"\n",(0,i.jsx)(n.h3,{id:"integration-architecture",children:"Integration Architecture"}),"\n",(0,i.jsx)(n.p,{children:"Multi-modal interaction systems integrate three primary modalities:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech"}),": Natural language commands and responses"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gesture"}),": Pointing, beckoning, and other body language"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision"}),": Object recognition, scene understanding, and visual context"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"These modalities work together to provide redundancy, disambiguation, and richer communication:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\nfrom typing import Dict, List, Tuple, Optional\r\nfrom dataclasses import dataclass\r\n\r\n@dataclass\r\nclass SpeechInput:\r\n    """Represents speech input with confidence and timing"""\r\n    text: str\r\n    confidence: float\r\n    timestamp: float\r\n    audio_data: Optional[np.ndarray] = None\r\n\r\n@dataclass\r\nclass GestureInput:\r\n    """Represents gesture input with type and location"""\r\n    gesture_type: str  # "pointing", "beckoning", "waving", etc.\r\n    location_2d: Tuple[int, int]  # 2D coordinates in camera frame\r\n    location_3d: Optional[Tuple[float, float, float]]  # 3D coordinates in world frame\r\n    confidence: float\r\n    timestamp: float\r\n\r\n@dataclass\r\nclass VisionInput:\r\n    """Represents visual input with detected objects and scene info"""\r\n    objects: List[Dict]  # List of detected objects with properties\r\n    scene_description: str\r\n    image: Optional[np.ndarray] = None\r\n    timestamp: float\r\n\r\nclass MultiModalInput:\r\n    """Container for synchronized multi-modal inputs"""\r\n    def __init__(self, speech: Optional[SpeechInput] = None,\r\n                 gesture: Optional[GestureInput] = None,\r\n                 vision: Optional[VisionInput] = None):\r\n        self.speech = speech\r\n        self.gesture = gesture\r\n        self.vision = vision\r\n        self.timestamp = max(\r\n            [ts for ts in [speech.timestamp if speech else None,\r\n                          gesture.timestamp if gesture else None,\r\n                          vision.timestamp if vision else None]\r\n             if ts is not None],\r\n            default=0.0\r\n        )\n'})}),"\n",(0,i.jsx)(n.h3,{id:"synchronization-and-fusion",children:"Synchronization and Fusion"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import time\r\nfrom collections import deque\r\n\r\nclass MultiModalSynchronizer:\r\n    def __init__(self, sync_window: float = 0.5):  # 500ms sync window\r\n        self.sync_window = sync_window\r\n        self.speech_buffer = deque(maxlen=10)\r\n        self.gesture_buffer = deque(maxlen=10)\r\n        self.vision_buffer = deque(maxlen=10)\r\n\r\n    def add_speech_input(self, speech: SpeechInput):\r\n        """Add speech input to synchronization buffer"""\r\n        self.speech_buffer.append(speech)\r\n        return self.attempt_synchronization()\r\n\r\n    def add_gesture_input(self, gesture: GestureInput):\r\n        """Add gesture input to synchronization buffer"""\r\n        self.gesture_buffer.append(gesture)\r\n        return self.attempt_synchronization()\r\n\r\n    def add_vision_input(self, vision: VisionInput):\r\n        """Add vision input to synchronization buffer"""\r\n        self.vision_buffer.append(vision)\r\n        return self.attempt_synchronization()\r\n\r\n    def attempt_synchronization(self) -> Optional[MultiModalInput]:\r\n        """Attempt to synchronize inputs within the time window"""\r\n        current_time = time.time()\r\n\r\n        # Find inputs within sync window\r\n        speech_input = self.find_recent_input(self.speech_buffer, current_time)\r\n        gesture_input = self.find_recent_input(self.gesture_buffer, current_time)\r\n        vision_input = self.find_recent_input(self.vision_buffer, current_time)\r\n\r\n        if speech_input or gesture_input or vision_input:\r\n            return MultiModalInput(speech_input, gesture_input, vision_input)\r\n\r\n        return None\r\n\r\n    def find_recent_input(self, buffer, reference_time):\r\n        """Find the most recent input within sync window"""\r\n        if not buffer:\r\n            return None\r\n\r\n        recent_input = None\r\n        min_time_diff = float(\'inf\')\r\n\r\n        for item in buffer:\r\n            time_diff = abs(reference_time - item.timestamp)\r\n            if time_diff <= self.sync_window and time_diff < min_time_diff:\r\n                min_time_diff = time_diff\r\n                recent_input = item\r\n\r\n        return recent_input\n'})}),"\n",(0,i.jsx)(n.h2,{id:"speech-processing-in-multi-modal-context",children:"Speech Processing in Multi-modal Context"}),"\n",(0,i.jsx)(n.h3,{id:"context-aware-speech-recognition",children:"Context-Aware Speech Recognition"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class ContextAwareSpeechProcessor:\r\n    def __init__(self, base_asr_model):\r\n        self.base_asr = base_asr_model\r\n        self.vision_context = None\r\n        self.gesture_context = None\r\n\r\n    def process_speech_with_context(self, audio_data, vision_context: VisionInput = None,\r\n                                  gesture_context: GestureInput = None) -> SpeechInput:\r\n        """Process speech with multi-modal context"""\r\n        self.vision_context = vision_context\r\n        self.gesture_context = gesture_context\r\n\r\n        # Get initial transcription\r\n        text = self.base_asr.transcribe(audio_data)\r\n        confidence = self.base_asr.get_confidence()\r\n\r\n        # Apply context-based disambiguation\r\n        disambiguated_text = self.apply_context_disambiguation(text)\r\n\r\n        # Update confidence based on context consistency\r\n        adjusted_confidence = self.adjust_confidence_with_context(\r\n            text, disambiguated_text, confidence\r\n        )\r\n\r\n        return SpeechInput(\r\n            text=disambiguated_text,\r\n            confidence=adjusted_confidence,\r\n            timestamp=time.time(),\r\n            audio_data=audio_data\r\n        )\r\n\r\n    def apply_context_disambiguation(self, text: str) -> str:\r\n        """Apply context to disambiguate speech"""\r\n        if not self.vision_context:\r\n            return text\r\n\r\n        # Example: Resolve pronouns based on visual context\r\n        # "Pick it up" -> "Pick up the red cup" (if red cup is visible)\r\n        if "it" in text.lower():\r\n            visible_objects = self.vision_context.objects\r\n            if visible_objects:\r\n                # For simplicity, take the most recently detected object\r\n                most_recent_object = visible_objects[0] if visible_objects else None\r\n                if most_recent_object:\r\n                    resolved_text = text.lower().replace("it", most_recent_object.get("name", "object"))\r\n                    return resolved_text\r\n\r\n        # Example: Resolve spatial references\r\n        # "Go there" -> "Go to the kitchen" (if gesture points to kitchen area)\r\n        if "there" in text.lower() and self.gesture_context:\r\n            if self.gesture_context.gesture_type == "pointing":\r\n                # Resolve "there" to actual location based on gesture\r\n                resolved_text = self.resolve_spatial_reference(text, self.gesture_context)\r\n                if resolved_text != text:\r\n                    return resolved_text\r\n\r\n        return text\r\n\r\n    def resolve_spatial_reference(self, text: str, gesture: GestureInput) -> str:\r\n        """Resolve spatial references like \'there\' based on gesture"""\r\n        # This would involve mapping 2D gesture location to 3D world coordinates\r\n        # and identifying the corresponding location or object\r\n        if gesture.location_3d:\r\n            # Convert 3D location to meaningful name (kitchen, living room, etc.)\r\n            location_name = self.get_location_name_from_coordinates(gesture.location_3d)\r\n            if location_name:\r\n                return text.lower().replace("there", f"to {location_name}")\r\n\r\n        return text\r\n\r\n    def adjust_confidence_with_context(self, original_text: str,\r\n                                     disambiguated_text: str,\r\n                                     base_confidence: float) -> float:\r\n        """Adjust confidence based on context consistency"""\r\n        if original_text == disambiguated_text:\r\n            # No disambiguation needed, keep original confidence\r\n            return base_confidence\r\n\r\n        # Disambiguation applied, potentially increase confidence if context is clear\r\n        if self.vision_context and self.gesture_context:\r\n            # Strong multi-modal context, increase confidence\r\n            return min(1.0, base_confidence + 0.1)\r\n\r\n        # Weak context, keep original confidence\r\n        return base_confidence\r\n\r\n    def get_location_name_from_coordinates(self, coordinates: Tuple[float, float, float]) -> Optional[str]:\r\n        """Convert 3D coordinates to meaningful location name"""\r\n        # This would typically use a map or spatial database\r\n        # For example, check if coordinates fall within known room boundaries\r\n        location_map = {\r\n            "kitchen": ((0, 0, 0), (3, 3, 3)),      # (min_coords, max_coords)\r\n            "living_room": ((3, 0, 0), (7, 4, 3)),\r\n            "bedroom": ((0, 4, 0), (4, 7, 3))\r\n        }\r\n\r\n        for location_name, (min_coords, max_coords) in location_map.items():\r\n            if (min_coords[0] <= coordinates[0] <= max_coords[0] and\r\n                min_coords[1] <= coordinates[1] <= max_coords[1] and\r\n                min_coords[2] <= coordinates[2] <= max_coords[2]):\r\n                return location_name\r\n\r\n        return None\n'})}),"\n",(0,i.jsx)(n.h2,{id:"gesture-recognition-and-interpretation",children:"Gesture Recognition and Interpretation"}),"\n",(0,i.jsx)(n.h3,{id:"gesture-detection-pipeline",children:"Gesture Detection Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mediapipe as mp\r\nimport math\r\n\r\nclass GestureRecognizer:\r\n    def __init__(self):\r\n        # Initialize MediaPipe for hand tracking\r\n        self.mp_hands = mp.solutions.hands\r\n        self.hands = self.mp_hands.Hands(\r\n            static_image_mode=False,\r\n            max_num_hands=2,\r\n            min_detection_confidence=0.7,\r\n            min_tracking_confidence=0.5\r\n        )\r\n        self.mp_drawing = mp.solutions.drawing_utils\r\n\r\n    def detect_gestures(self, image: np.ndarray) -> List[GestureInput]:\r\n        """Detect gestures from image frame"""\r\n        results = self.hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\r\n        gestures = []\r\n\r\n        if results.multi_hand_landmarks:\r\n            for hand_landmarks in results.multi_hand_landmarks:\r\n                gesture = self.classify_gesture(hand_landmarks, image.shape)\r\n                if gesture:\r\n                    gestures.append(gesture)\r\n\r\n        return gestures\r\n\r\n    def classify_gesture(self, hand_landmarks, image_shape) -> Optional[GestureInput]:\r\n        """Classify hand gesture based on landmarks"""\r\n        # Get key landmarks\r\n        landmarks = hand_landmarks.landmark\r\n\r\n        # Calculate gesture features\r\n        pointing_direction = self.calculate_pointing_direction(landmarks, image_shape)\r\n        finger_positions = self.get_finger_positions(landmarks)\r\n        palm_orientation = self.get_palm_orientation(landmarks)\r\n\r\n        # Classify gesture type\r\n        gesture_type = self.classify_gesture_type(finger_positions, pointing_direction)\r\n\r\n        if gesture_type:\r\n            # Calculate 2D position (tip of index finger)\r\n            index_finger_tip = landmarks[self.mp_hands.HandLandmark.INDEX_FINGER_TIP]\r\n            x_2d = int(index_finger_tip.x * image_shape[1])\r\n            y_2d = int(index_finger_tip.y * image_shape[0])\r\n\r\n            return GestureInput(\r\n                gesture_type=gesture_type,\r\n                location_2d=(x_2d, y_2d),\r\n                location_3d=None,  # Will be calculated later with depth info\r\n                confidence=0.8,  # Base confidence\r\n                timestamp=time.time()\r\n            )\r\n\r\n        return None\r\n\r\n    def calculate_pointing_direction(self, landmarks, image_shape) -> Tuple[float, float]:\r\n        """Calculate the pointing direction of the hand"""\r\n        # Use index finger tip and wrist to determine pointing direction\r\n        wrist = landmarks[self.mp_hands.HandLandmark.WRIST]\r\n        index_tip = landmarks[self.mp_hands.HandLandmark.INDEX_FINGER_TIP]\r\n\r\n        dx = index_tip.x - wrist.x\r\n        dy = index_tip.y - wrist.y\r\n\r\n        # Normalize\r\n        length = math.sqrt(dx*dx + dy*dy)\r\n        if length > 0:\r\n            return (dx/length, dy/length)\r\n        else:\r\n            return (0, 0)\r\n\r\n    def get_finger_positions(self, landmarks) -> Dict[str, Tuple[float, float]]:\r\n        """Get positions of key fingers"""\r\n        finger_tips = {\r\n            \'thumb\': landmarks[self.mp_hands.HandLandmark.THUMB_TIP],\r\n            \'index\': landmarks[self.mp_hands.HandLandmark.INDEX_FINGER_TIP],\r\n            \'middle\': landmarks[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP],\r\n            \'ring\': landmarks[self.mp_hands.HandLandmark.RING_FINGER_TIP],\r\n            \'pinky\': landmarks[self.mp_hands.HandLandmark.PINKY_TIP]\r\n        }\r\n\r\n        return {finger: (landmark.x, landmark.y) for finger, landmark in finger_tips.items()}\r\n\r\n    def get_palm_orientation(self, landmarks) -> float:\r\n        """Calculate palm orientation (simplified)"""\r\n        # Use wrist and middle finger metacarpal to estimate palm orientation\r\n        wrist = landmarks[self.mp_hands.HandLandmark.WRIST]\r\n        middle_mcp = landmarks[self.mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\r\n\r\n        # Calculate angle\r\n        dx = middle_mcp.x - wrist.x\r\n        dy = middle_mcp.y - wrist.y\r\n\r\n        return math.atan2(dy, dx)\r\n\r\n    def classify_gesture_type(self, finger_positions: Dict, pointing_direction) -> Optional[str]:\r\n        """Classify gesture type based on finger positions"""\r\n        # Simple classification based on finger positions\r\n        index_pos = finger_positions[\'index\']\r\n        middle_pos = finger_positions[\'middle\']\r\n        thumb_pos = finger_positions[\'thumb\']\r\n        ring_pos = finger_positions[\'ring\']\r\n        pinky_pos = finger_positions[\'pinky\']\r\n\r\n        # Pointing gesture: index finger extended, others curled\r\n        if (self.is_extended(index_pos, thumb_pos) and\r\n            self.is_curled(middle_pos, thumb_pos) and\r\n            self.is_curled(ring_pos, thumb_pos) and\r\n            self.is_curled(pinky_pos, thumb_pos)):\r\n            return "pointing"\r\n\r\n        # Peace sign: index and middle extended, others curled\r\n        if (self.is_extended(index_pos, thumb_pos) and\r\n            self.is_extended(middle_pos, thumb_pos) and\r\n            self.is_curled(ring_pos, thumb_pos) and\r\n            self.is_curled(pinky_pos, thumb_pos)):\r\n            return "peace"\r\n\r\n        # Thumbs up: thumb up, others curled\r\n        if (self.is_extended(thumb_pos, finger_positions[\'index\']) and\r\n            self.is_curled(index_pos, thumb_pos) and\r\n            self.is_curled(middle_pos, thumb_pos)):\r\n            return "thumbs_up"\r\n\r\n        # Wave: all fingers extended, moving\r\n        if (self.are_all_extended([index_pos, middle_pos, ring_pos, pinky_pos], thumb_pos)):\r\n            return "waving"\r\n\r\n        return None\r\n\r\n    def is_extended(self, finger_pos, reference_pos, threshold=0.1) -> bool:\r\n        """Check if finger is extended (away from palm)"""\r\n        distance = math.sqrt((finger_pos[0] - reference_pos[0])**2 +\r\n                           (finger_pos[1] - reference_pos[1])**2)\r\n        return distance > threshold\r\n\r\n    def is_curled(self, finger_pos, reference_pos, threshold=0.1) -> bool:\r\n        """Check if finger is curled (close to palm)"""\r\n        return not self.is_extended(finger_pos, reference_pos, threshold)\r\n\r\n    def are_all_extended(self, finger_positions, reference_pos, threshold=0.1) -> bool:\r\n        """Check if all fingers are extended"""\r\n        return all(self.is_extended(pos, reference_pos, threshold)\r\n                  for pos in finger_positions)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3d-gesture-mapping",children:"3D Gesture Mapping"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class Gesture3DMapper:\r\n    def __init__(self, camera_intrinsics, robot_pose):\r\n        self.camera_intrinsics = camera_intrinsics  # Camera parameters\r\n        self.robot_pose = robot_pose  # Robot\'s current pose in world\r\n\r\n    def map_2d_to_3d(self, gesture_2d: GestureInput, depth_image: np.ndarray) -> GestureInput:\r\n        """Map 2D gesture coordinates to 3D world coordinates"""\r\n        x_2d, y_2d = gesture_2d.location_2d\r\n\r\n        # Get depth at gesture location\r\n        if depth_image is not None:\r\n            depth_value = depth_image[y_2d, x_2d]\r\n        else:\r\n            # Use estimated depth or default value\r\n            depth_value = 1.0  # meters\r\n\r\n        # Convert 2D pixel coordinates + depth to 3D world coordinates\r\n        x_3d, y_3d, z_3d = self.pixel_to_world_coordinates(\r\n            x_2d, y_2d, depth_value, self.camera_intrinsics\r\n        )\r\n\r\n        # Transform to robot\'s coordinate frame\r\n        world_coords = self.transform_to_robot_frame(\r\n            (x_3d, y_3d, z_3d), self.robot_pose\r\n        )\r\n\r\n        # Update gesture with 3D coordinates\r\n        gesture_3d = GestureInput(\r\n            gesture_type=gesture_2d.gesture_type,\r\n            location_2d=gesture_2d.location_2d,\r\n            location_3d=world_coords,\r\n            confidence=gesture_2d.confidence,\r\n            timestamp=gesture_2d.timestamp\r\n        )\r\n\r\n        return gesture_3d\r\n\r\n    def pixel_to_world_coordinates(self, x: int, y: int, depth: float,\r\n                                 intrinsics: Dict) -> Tuple[float, float, float]:\r\n        """Convert pixel coordinates + depth to world coordinates"""\r\n        # Camera intrinsic parameters\r\n        fx = intrinsics[\'fx\']\r\n        fy = intrinsics[\'fy\']\r\n        cx = intrinsics[\'cx\']\r\n        cy = intrinsics[\'cy\']\r\n\r\n        # Convert to normalized coordinates\r\n        x_norm = (x - cx) / fx\r\n        y_norm = (y - cy) / fy\r\n\r\n        # Convert to world coordinates\r\n        x_world = x_norm * depth\r\n        y_world = y_norm * depth\r\n        z_world = depth\r\n\r\n        return (x_world, y_world, z_world)\r\n\r\n    def transform_to_robot_frame(self, world_coords: Tuple[float, float, float],\r\n                               robot_pose: Dict) -> Tuple[float, float, float]:\r\n        """Transform world coordinates to robot\'s local coordinate frame"""\r\n        # This would involve applying the inverse of the robot\'s pose transformation\r\n        # For simplicity, returning the same coordinates\r\n        # In practice, this would involve rotation and translation matrices\r\n        return world_coords\n'})}),"\n",(0,i.jsx)(n.h2,{id:"vision-processing-and-scene-understanding",children:"Vision Processing and Scene Understanding"}),"\n",(0,i.jsx)(n.h3,{id:"object-detection-and-recognition",children:"Object Detection and Recognition"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torchvision.transforms as T\r\nfrom PIL import Image\r\n\r\nclass VisionProcessor:\r\n    def __init__(self):\r\n        # Load pre-trained object detection model (e.g., YOLO, Faster R-CNN)\r\n        self.detection_model = self.load_detection_model()\r\n        self.classification_model = self.load_classification_model()\r\n        self.transform = T.Compose([\r\n            T.ToTensor(),\r\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n        ])\r\n\r\n    def load_detection_model(self):\r\n        \"\"\"Load object detection model\"\"\"\r\n        # Using torchvision's pre-trained model as an example\r\n        model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\r\n        return model\r\n\r\n    def load_classification_model(self):\r\n        \"\"\"Load object classification model\"\"\"\r\n        # Example using ResNet\r\n        model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\r\n        model.eval()\r\n        return model\r\n\r\n    def process_scene(self, image: np.ndarray) -> VisionInput:\r\n        \"\"\"Process image to detect objects and understand scene\"\"\"\r\n        # Convert numpy array to PIL Image\r\n        pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\r\n\r\n        # Run object detection\r\n        results = self.detection_model(pil_image)\r\n\r\n        # Extract detected objects\r\n        objects = self.extract_detected_objects(results, image.shape)\r\n\r\n        # Generate scene description\r\n        scene_description = self.generate_scene_description(objects, image)\r\n\r\n        return VisionInput(\r\n            objects=objects,\r\n            scene_description=scene_description,\r\n            image=image,\r\n            timestamp=time.time()\r\n        )\r\n\r\n    def extract_detected_objects(self, detection_results, image_shape) -> List[Dict]:\r\n        \"\"\"Extract objects from detection results\"\"\"\r\n        objects = []\r\n\r\n        # Parse YOLO results\r\n        detections = detection_results.pandas().xyxy[0]\r\n\r\n        for _, detection in detections.iterrows():\r\n            object_info = {\r\n                'name': detection['name'],\r\n                'confidence': detection['confidence'],\r\n                'bbox': {\r\n                    'x_min': int(detection['xmin']),\r\n                    'y_min': int(detection['ymin']),\r\n                    'x_max': int(detection['xmax']),\r\n                    'y_max': int(detection['ymax'])\r\n                },\r\n                'center_2d': (\r\n                    int((detection['xmin'] + detection['xmax']) / 2),\r\n                    int((detection['ymin'] + detection['ymax']) / 2)\r\n                ),\r\n                'area': (detection['xmax'] - detection['xmin']) * (detection['ymax'] - detection['ymin'])\r\n            }\r\n\r\n            # Estimate 3D information if depth is available\r\n            if hasattr(self, 'depth_estimator'):\r\n                object_info['center_3d'] = self.estimate_3d_position(object_info, image_shape)\r\n\r\n            objects.append(object_info)\r\n\r\n        return objects\r\n\r\n    def generate_scene_description(self, objects: List[Dict], image: np.ndarray) -> str:\r\n        \"\"\"Generate textual description of the scene\"\"\"\r\n        if not objects:\r\n            return \"The scene appears to be empty or no objects were detected.\"\r\n\r\n        # Count objects by type\r\n        object_counts = {}\r\n        for obj in objects:\r\n            name = obj['name']\r\n            object_counts[name] = object_counts.get(name, 0) + 1\r\n\r\n        # Create description\r\n        description_parts = []\r\n        for obj_name, count in object_counts.items():\r\n            if count == 1:\r\n                description_parts.append(f\"a {obj_name}\")\r\n            else:\r\n                description_parts.append(f\"{count} {obj_name}s\")\r\n\r\n        object_list = \", \".join(description_parts[:-1])\r\n        if len(description_parts) > 1:\r\n            scene_desc = f\"The scene contains {object_list}, and {description_parts[-1]}.\"\r\n        else:\r\n            scene_desc = f\"The scene contains {description_parts[0]}.\"\r\n\r\n        return scene_desc\r\n\r\n    def estimate_3d_position(self, object_info: Dict, image_shape: tuple) -> Optional[Tuple[float, float, float]]:\r\n        \"\"\"Estimate 3D position of object (requires depth information)\"\"\"\r\n        # This would typically use depth estimation or stereo vision\r\n        # For now, returning None\r\n        return None\r\n\r\n    def get_object_at_location(self, location_2d: Tuple[int, int], objects: List[Dict]) -> Optional[Dict]:\r\n        \"\"\"Get object at or near specified 2D location\"\"\"\r\n        x, y = location_2d\r\n\r\n        for obj in objects:\r\n            bbox = obj['bbox']\r\n            if (bbox['x_min'] <= x <= bbox['x_max'] and\r\n                bbox['y_min'] <= y <= bbox['y_max']):\r\n                return obj\r\n\r\n        # If no object exactly at location, find closest one\r\n        closest_obj = None\r\n        min_distance = float('inf')\r\n\r\n        for obj in objects:\r\n            center = obj['center_2d']\r\n            distance = math.sqrt((center[0] - x)**2 + (center[1] - y)**2)\r\n            if distance < min_distance:\r\n                min_distance = distance\r\n                closest_obj = obj\r\n\r\n        return closest_obj\n"})}),"\n",(0,i.jsx)(n.h2,{id:"multi-modal-fusion-and-interpretation",children:"Multi-modal Fusion and Interpretation"}),"\n",(0,i.jsx)(n.h3,{id:"fusion-engine",children:"Fusion Engine"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class MultiModalFusionEngine:\r\n    def __init__(self):\r\n        self.confidence_weights = {\r\n            'speech': 0.4,\r\n            'gesture': 0.3,\r\n            'vision': 0.3\r\n        }\r\n\r\n    def fuse_inputs(self, multi_modal_input: MultiModalInput) -> Dict:\r\n        \"\"\"Fuse multi-modal inputs into coherent interpretation\"\"\"\r\n        interpretations = {}\r\n\r\n        # Process each modality\r\n        if multi_modal_input.speech:\r\n            interpretations['speech'] = self.interpret_speech(multi_modal_input.speech)\r\n\r\n        if multi_modal_input.gesture:\r\n            interpretations['gesture'] = self.interpret_gesture(multi_modal_input.gesture)\r\n\r\n        if multi_modal_input.vision:\r\n            interpretations['vision'] = self.interpret_vision(multi_modal_input.vision)\r\n\r\n        # Fuse interpretations based on confidence and context\r\n        fused_interpretation = self.fuse_interpretations(interpretations, multi_modal_input)\r\n\r\n        return fused_interpretation\r\n\r\n    def interpret_speech(self, speech: SpeechInput) -> Dict:\r\n        \"\"\"Interpret speech input\"\"\"\r\n        # This would use NLU system from previous sections\r\n        interpretation = {\r\n            'intent': self.extract_intent(speech.text),\r\n            'entities': self.extract_entities(speech.text),\r\n            'confidence': speech.confidence,\r\n            'modality': 'speech'\r\n        }\r\n        return interpretation\r\n\r\n    def interpret_gesture(self, gesture: GestureInput) -> Dict:\r\n        \"\"\"Interpret gesture input\"\"\"\r\n        interpretation = {\r\n            'gesture_type': gesture.gesture_type,\r\n            'target_location': gesture.location_3d or gesture.location_2d,\r\n            'confidence': gesture.confidence,\r\n            'modality': 'gesture'\r\n        }\r\n        return interpretation\r\n\r\n    def interpret_vision(self, vision: VisionInput) -> Dict:\r\n        \"\"\"Interpret vision input\"\"\"\r\n        interpretation = {\r\n            'detected_objects': vision.objects,\r\n            'scene_description': vision.scene_description,\r\n            'modality': 'vision'\r\n        }\r\n        return interpretation\r\n\r\n    def extract_intent(self, text: str) -> str:\r\n        \"\"\"Extract intent from text (simplified)\"\"\"\r\n        # This would use proper NLU system\r\n        if any(word in text.lower() for word in ['go', 'move', 'navigate', 'walk']):\r\n            return 'navigation'\r\n        elif any(word in text.lower() for word in ['pick', 'grasp', 'take', 'get']):\r\n            return 'manipulation'\r\n        elif any(word in text.lower() for word in ['find', 'look', 'search']):\r\n            return 'perception'\r\n        else:\r\n            return 'unknown'\r\n\r\n    def extract_entities(self, text: str) -> List[str]:\r\n        \"\"\"Extract entities from text (simplified)\"\"\"\r\n        # This would use proper NER system\r\n        return []\r\n\r\n    def fuse_interpretations(self, interpretations: Dict, multi_modal_input: MultiModalInput) -> Dict:\r\n        \"\"\"Fuse interpretations from different modalities\"\"\"\r\n        # Initialize result structure\r\n        fused_result = {\r\n            'primary_intent': None,\r\n            'target_object': None,\r\n            'target_location': None,\r\n            'action': None,\r\n            'confidence': 0.0,\r\n            'modality_contributions': {}\r\n        }\r\n\r\n        # Determine primary intent based on modality agreement and confidence\r\n        intent_confidence = {}\r\n\r\n        if 'speech' in interpretations:\r\n            speech_intent = interpretations['speech']['intent']\r\n            intent_confidence[speech_intent] = interpretations['speech']['confidence'] * self.confidence_weights['speech']\r\n\r\n        if 'gesture' in interpretations:\r\n            # Gestures often indicate navigation or pointing\r\n            gesture_intent = 'navigation' if interpretations['gesture']['gesture_type'] == 'pointing' else 'attention'\r\n            gesture_conf = interpretations['gesture']['confidence'] * self.confidence_weights['gesture']\r\n            intent_confidence[gesture_intent] = intent_confidence.get(gesture_intent, 0) + gesture_conf\r\n\r\n        if 'vision' in interpretations:\r\n            # Vision provides context but less direct intent\r\n            vision_conf = 0.5 * self.confidence_weights['vision']  # Base confidence\r\n            intent_confidence['context'] = vision_conf\r\n\r\n        # Select primary intent with highest confidence\r\n        if intent_confidence:\r\n            fused_result['primary_intent'] = max(intent_confidence, key=intent_confidence.get)\r\n            fused_result['confidence'] = intent_confidence[fused_result['primary_intent']]\r\n\r\n        # Resolve target object using multi-modal information\r\n        fused_result['target_object'] = self.resolve_target_object(interpretations, multi_modal_input)\r\n\r\n        # Resolve target location using gesture and vision\r\n        fused_result['target_location'] = self.resolve_target_location(interpretations, multi_modal_input)\r\n\r\n        # Generate action based on fused information\r\n        fused_result['action'] = self.generate_action(fused_result)\r\n\r\n        # Store modality contributions\r\n        fused_result['modality_contributions'] = {k: v for k, v in intent_confidence.items()}\r\n\r\n        return fused_result\r\n\r\n    def resolve_target_object(self, interpretations: Dict, multi_modal_input: MultiModalInput) -> Optional[str]:\r\n        \"\"\"Resolve target object using multi-modal information\"\"\"\r\n        candidates = []\r\n\r\n        # Get object mentions from speech\r\n        if 'speech' in interpretations:\r\n            # This would use more sophisticated NLU to extract object mentions\r\n            speech_objects = self.extract_objects_from_speech(interpretations['speech']['entities'])\r\n            candidates.extend(speech_objects)\r\n\r\n        # Get object at gesture location\r\n        if 'gesture' in interpretations and 'vision' in interpretations:\r\n            gesture_location = interpretations['gesture'].get('target_location')\r\n            if gesture_location and multi_modal_input.vision:\r\n                if isinstance(gesture_location, tuple) and len(gesture_location) == 2:\r\n                    # 2D coordinates\r\n                    target_obj = self.vision_processor.get_object_at_location(\r\n                        gesture_location, multi_modal_input.vision.objects\r\n                    )\r\n                    if target_obj:\r\n                        candidates.append(target_obj['name'])\r\n\r\n        # Get most salient object from vision\r\n        if 'vision' in interpretations:\r\n            vision_objects = interpretations['vision']['detected_objects']\r\n            if vision_objects:\r\n                # Select most prominent object (largest area, closest, etc.)\r\n                most_salient = max(vision_objects, key=lambda x: x['area'])\r\n                candidates.append(most_salient['name'])\r\n\r\n        # Return most confident candidate\r\n        return candidates[0] if candidates else None\r\n\r\n    def resolve_target_location(self, interpretations: Dict, multi_modal_input: MultiModalInput) -> Optional[Tuple]:\r\n        \"\"\"Resolve target location using multi-modal information\"\"\"\r\n        if 'gesture' in interpretations:\r\n            gesture = interpretations['gesture']\r\n            if gesture.get('target_location'):\r\n                return gesture['target_location']\r\n\r\n        if 'vision' in interpretations:\r\n            # Use scene description to infer location\r\n            scene_desc = interpretations['vision']['scene_description']\r\n            # This would use more sophisticated spatial reasoning\r\n            return self.infer_location_from_scene(scene_desc)\r\n\r\n        return None\r\n\r\n    def generate_action(self, fused_result: Dict) -> Optional[str]:\r\n        \"\"\"Generate appropriate action based on fused interpretation\"\"\"\r\n        intent = fused_result['primary_intent']\r\n        obj = fused_result['target_object']\r\n        location = fused_result['target_location']\r\n\r\n        if intent == 'navigation' and location:\r\n            return f\"navigate_to({location})\"\r\n        elif intent == 'manipulation' and obj:\r\n            return f\"manipulate_object({obj})\"\r\n        elif intent == 'perception' and obj:\r\n            return f\"perceive_object({obj})\"\r\n        else:\r\n            return \"wait_for_clarification\"\r\n\r\n    def extract_objects_from_speech(self, entities: List[str]) -> List[str]:\r\n        \"\"\"Extract object names from speech entities\"\"\"\r\n        # Simplified implementation\r\n        return entities\r\n\r\n    def infer_location_from_scene(self, scene_description: str) -> Optional[Tuple]:\r\n        \"\"\"Infer location from scene description\"\"\"\r\n        # This would use more sophisticated spatial reasoning\r\n        # For now, returning None\r\n        return None\n"})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-robot-control",children:"Integration with Robot Control"}),"\n",(0,i.jsx)(n.h3,{id:"action-execution-system",children:"Action Execution System"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.action import ActionClient\r\nfrom geometry_msgs.msg import Pose, Point\r\nfrom std_msgs.msg import String\r\n\r\nclass MultiModalActionExecutor:\r\n    def __init__(self):\r\n        self.fusion_engine = MultiModalFusionEngine()\r\n        self.vision_processor = VisionProcessor()\r\n        self.gesture_recognizer = GestureRecognizer()\r\n        self.speech_processor = ContextAwareSpeechProcessor(None)  # Placeholder for ASR\r\n\r\n        # ROS 2 setup\r\n        rclpy.init()\r\n        self.node = rclpy.create_node(\'multimodal_executor\')\r\n\r\n        # Action clients\r\n        self.move_base_client = ActionClient(self.node, \'MoveBase\', \'move_base\')\r\n        self.manipulation_client = ActionClient(self.node, \'Manipulate\', \'manipulation\')\r\n\r\n        # Publishers\r\n        self.speech_publisher = self.node.create_publisher(String, \'robot_speech\', 10)\r\n\r\n    def process_multi_modal_command(self, multi_modal_input: MultiModalInput) -> bool:\r\n        """Process multi-modal input and execute corresponding action"""\r\n        try:\r\n            # Fuse the multi-modal inputs\r\n            fused_interpretation = self.fusion_engine.fuse_inputs(multi_modal_input)\r\n\r\n            # Validate interpretation confidence\r\n            if fused_interpretation[\'confidence\'] < 0.5:\r\n                self.request_clarification(multi_modal_input)\r\n                return False\r\n\r\n            # Execute action based on interpretation\r\n            action_result = self.execute_action(fused_interpretation)\r\n\r\n            return action_result\r\n\r\n        except Exception as e:\r\n            self.node.get_logger().error(f\'Error processing multi-modal command: {e}\')\r\n            return False\r\n\r\n    def execute_action(self, interpretation: Dict) -> bool:\r\n        """Execute robot action based on interpretation"""\r\n        action = interpretation[\'action\']\r\n\r\n        if action.startswith(\'navigate_to\'):\r\n            return self.execute_navigation(interpretation)\r\n        elif action.startswith(\'manipulate_object\'):\r\n            return self.execute_manipulation(interpretation)\r\n        elif action.startswith(\'perceive_object\'):\r\n            return self.execute_perception(interpretation)\r\n        else:\r\n            self.node.get_logger().warn(f\'Unknown action: {action}\')\r\n            return False\r\n\r\n    def execute_navigation(self, interpretation: Dict) -> bool:\r\n        """Execute navigation action"""\r\n        target_location = interpretation[\'target_location\']\r\n\r\n        if not target_location:\r\n            self.node.get_logger().error(\'No target location specified for navigation\')\r\n            return False\r\n\r\n        # Convert location to ROS Pose\r\n        pose = self.convert_location_to_pose(target_location)\r\n\r\n        # Send navigation goal\r\n        goal_msg = self.create_navigation_goal(pose)\r\n\r\n        self.move_base_client.wait_for_server()\r\n        future = self.move_base_client.send_goal(goal_msg)\r\n\r\n        # Wait for result\r\n        rclpy.spin_until_future_complete(self.node, future)\r\n\r\n        return future.result().success\r\n\r\n    def execute_manipulation(self, interpretation: Dict) -> bool:\r\n        """Execute manipulation action"""\r\n        target_object = interpretation[\'target_object\']\r\n\r\n        if not target_object:\r\n            self.node.get_logger().error(\'No target object specified for manipulation\')\r\n            return False\r\n\r\n        # Find object in current scene\r\n        current_objects = self.get_current_objects()\r\n        target_obj_info = next((obj for obj in current_objects if obj[\'name\'] == target_object), None)\r\n\r\n        if not target_obj_info:\r\n            self.node.get_logger().error(f\'Object {target_object} not found in current scene\')\r\n            return False\r\n\r\n        # Create manipulation goal\r\n        goal_msg = self.create_manipulation_goal(target_obj_info)\r\n\r\n        self.manipulation_client.wait_for_server()\r\n        future = self.manipulation_client.send_goal(goal_msg)\r\n\r\n        # Wait for result\r\n        rclpy.spin_until_future_complete(self.node, future)\r\n\r\n        return future.result().success\r\n\r\n    def execute_perception(self, interpretation: Dict) -> bool:\r\n        """Execute perception action"""\r\n        target_object = interpretation[\'target_object\']\r\n\r\n        if target_object:\r\n            # Focus perception on specific object\r\n            self.focus_perception_on_object(target_object)\r\n        else:\r\n            # Perform general scene perception\r\n            self.perform_general_perception()\r\n\r\n        return True\r\n\r\n    def request_clarification(self, multi_modal_input: MultiModalInput):\r\n        """Request clarification when interpretation confidence is low"""\r\n        # Generate clarification request\r\n        request = self.generate_clarification_request(multi_modal_input)\r\n\r\n        # Output request (speech, display, etc.)\r\n        self.output_clarification_request(request)\r\n\r\n    def generate_clarification_request(self, multi_modal_input: MultiModalInput) -> str:\r\n        """Generate natural language clarification request"""\r\n        request_parts = ["I\'m not sure I understood correctly. Did you mean"]\r\n\r\n        if multi_modal_input.speech:\r\n            request_parts.append(f"to {multi_modal_input.speech.text}")\r\n\r\n        if multi_modal_input.gesture:\r\n            request_parts.append(f"to go to where you pointed")\r\n\r\n        if multi_modal_input.vision:\r\n            objects = [obj[\'name\'] for obj in multi_modal_input.vision.objects[:2]]  # First 2 objects\r\n            if objects:\r\n                request_parts.append(f"to interact with the " + " or ".join(objects))\r\n\r\n        return " or ".join(request_parts) + "?"\r\n\r\n    def output_clarification_request(self, request: str):\r\n        """Output clarification request to user"""\r\n        # Publish to speech system\r\n        msg = String()\r\n        msg.data = request\r\n        self.speech_publisher.publish(msg)\r\n\r\n        print(f"Robot: {request}")\r\n\r\n    def get_current_objects(self) -> List[Dict]:\r\n        """Get currently detected objects"""\r\n        # This would interface with current vision system\r\n        # For now, returning empty list\r\n        return []\r\n\r\n    def focus_perception_on_object(self, object_name: str):\r\n        """Focus perception system on specific object"""\r\n        # Implementation would direct cameras, adjust focus, etc.\r\n        pass\r\n\r\n    def perform_general_perception(self):\r\n        """Perform general scene perception"""\r\n        # Implementation would scan environment, detect objects, etc.\r\n        pass\r\n\r\n    def convert_location_to_pose(self, location: Tuple) -> Pose:\r\n        """Convert location coordinates to ROS Pose"""\r\n        pose = Pose()\r\n        if len(location) >= 3:\r\n            pose.position = Point(x=location[0], y=location[1], z=location[2])\r\n        else:\r\n            pose.position = Point(x=location[0], y=location[1], z=0.0)\r\n        return pose\r\n\r\n    def create_navigation_goal(self, pose: Pose):\r\n        """Create navigation goal message"""\r\n        # Implementation depends on specific navigation system\r\n        pass\r\n\r\n    def create_manipulation_goal(self, object_info: Dict):\r\n        """Create manipulation goal message"""\r\n        # Implementation depends on specific manipulation system\r\n        pass\n'})}),"\n",(0,i.jsx)(n.h2,{id:"real-time-processing-and-optimization",children:"Real-time Processing and Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"efficient-multi-modal-pipeline",children:"Efficient Multi-modal Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import threading\r\nimport asyncio\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\nclass EfficientMultiModalPipeline:\r\n    def __init__(self):\r\n        self.speech_processor = ContextAwareSpeechProcessor(None)\r\n        self.gesture_recognizer = GestureRecognizer()\r\n        self.vision_processor = VisionProcessor()\r\n        self.fusion_engine = MultiModalFusionEngine()\r\n        self.action_executor = MultiModalActionExecutor()\r\n\r\n        # Threading for parallel processing\r\n        self.executor = ThreadPoolExecutor(max_workers=4)\r\n        self.speech_lock = threading.Lock()\r\n        self.vision_lock = threading.Lock()\r\n\r\n    def process_frame_async(self, frame: np.ndarray, audio_chunk: Optional[np.ndarray] = None):\r\n        """Process video frame and audio chunk asynchronously"""\r\n        # Process vision in parallel\r\n        vision_future = self.executor.submit(self.vision_processor.process_scene, frame)\r\n\r\n        # Process audio if available\r\n        speech_future = None\r\n        if audio_chunk is not None:\r\n            speech_future = self.executor.submit(\r\n                self.speech_processor.process_speech_with_context,\r\n                audio_chunk\r\n            )\r\n\r\n        # Process gestures from frame\r\n        gesture_future = self.executor.submit(self.gesture_recognizer.detect_gestures, frame)\r\n\r\n        # Wait for results\r\n        vision_result = vision_future.result()\r\n        gesture_results = gesture_future.result()\r\n        speech_result = speech_future.result() if speech_future else None\r\n\r\n        # Synchronize and fuse results\r\n        if gesture_results:\r\n            # Use first detected gesture for now\r\n            gesture_input = gesture_results[0]\r\n        else:\r\n            gesture_input = None\r\n\r\n        multi_modal_input = MultiModalInput(\r\n            speech=speech_result,\r\n            gesture=gesture_input,\r\n            vision=vision_result\r\n        )\r\n\r\n        # Fuse and execute\r\n        return self.action_executor.process_multi_modal_command(multi_modal_input)\r\n\r\n    def run_continuous_processing(self):\r\n        """Run continuous multi-modal processing"""\r\n        import cv2\r\n\r\n        cap = cv2.VideoCapture(0)  # Camera input\r\n        # audio_stream = initialize_audio_stream()  # Audio input\r\n\r\n        try:\r\n            while True:\r\n                ret, frame = cap.read()\r\n                if not ret:\r\n                    continue\r\n\r\n                # Get audio chunk (simplified)\r\n                audio_chunk = None  # get_audio_chunk()\r\n\r\n                # Process frame asynchronously\r\n                result = self.process_frame_async(frame, audio_chunk)\r\n\r\n                # Display results\r\n                cv2.imshow(\'Multi-modal Processing\', frame)\r\n\r\n                if cv2.waitKey(1) & 0xFF == ord(\'q\'):\r\n                    break\r\n\r\n        finally:\r\n            cap.release()\r\n            cv2.destroyAllWindows()\n'})}),"\n",(0,i.jsx)(n.p,{children:"Multi-modal interaction systems combine speech, gesture, and vision to create natural, intuitive human-robot communication that leverages the strengths of each modality while providing redundancy and disambiguation for robust interaction."})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var r=t(6540);const i={},o=r.createContext(i);function s(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);