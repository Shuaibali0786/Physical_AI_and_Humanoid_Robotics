"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[477],{3297:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>_,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module3-isaac/reinforcement-learning","title":"Reinforcement Learning in Robotics with Isaac Sim","description":"Introduction to RL for Robotics","source":"@site/docs/module3-isaac/reinforcement-learning.md","sourceDirName":"module3-isaac","slug":"/module3-isaac/reinforcement-learning","permalink":"/docs/module3-isaac/reinforcement-learning","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"modules","previous":{"title":"Perception and Manipulation with Isaac Sim","permalink":"/docs/module3-isaac/perception-manipulation"},"next":{"title":"Sim-to-Real Transfer","permalink":"/docs/module3-isaac/sim-to-real"}}');var s=r(4848),i=r(8453);const a={},o="Reinforcement Learning in Robotics with Isaac Sim",c={},l=[{value:"Introduction to RL for Robotics",id:"introduction-to-rl-for-robotics",level:2},{value:"RL Fundamentals for Robotics",id:"rl-fundamentals-for-robotics",level:2},{value:"Markov Decision Processes in Robotics",id:"markov-decision-processes-in-robotics",level:3},{value:"RL Problem Formulation",id:"rl-problem-formulation",level:3},{value:"Isaac Sim RL Integration",id:"isaac-sim-rl-integration",level:2},{value:"Isaac Gym Environment",id:"isaac-gym-environment",level:3},{value:"Physics Simulation for RL",id:"physics-simulation-for-rl",level:3},{value:"Deep RL Algorithms for Robotics",id:"deep-rl-algorithms-for-robotics",level:2},{value:"Deep Deterministic Policy Gradient (DDPG)",id:"deep-deterministic-policy-gradient-ddpg",level:3},{value:"Soft Actor-Critic (SAC)",id:"soft-actor-critic-sac",level:3},{value:"Proximal Policy Optimization (PPO)",id:"proximal-policy-optimization-ppo",level:3},{value:"Isaac Sim RL Examples",id:"isaac-sim-rl-examples",level:2},{value:"Humanoid Locomotion",id:"humanoid-locomotion",level:3},{value:"Manipulation Tasks",id:"manipulation-tasks",level:3},{value:"Sample Efficiency and Transfer Learning",id:"sample-efficiency-and-transfer-learning",level:2},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Curriculum Learning",id:"curriculum-learning",level:3},{value:"Isaac Sim RL Training Pipeline",id:"isaac-sim-rl-training-pipeline",level:2},{value:"Training Configuration",id:"training-configuration",level:3},{value:"Multi-Environment Training",id:"multi-environment-training",level:3},{value:"Safety and Robustness",id:"safety-and-robustness",level:2},{value:"Safe RL in Isaac Sim",id:"safe-rl-in-isaac-sim",level:3},{value:"Robustness Testing",id:"robustness-testing",level:3},{value:"Advanced RL Techniques",id:"advanced-rl-techniques",level:2},{value:"Meta-Learning for Robotics",id:"meta-learning-for-robotics",level:3},{value:"Multi-Task RL",id:"multi-task-rl",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"reinforcement-learning-in-robotics-with-isaac-sim",children:"Reinforcement Learning in Robotics with Isaac Sim"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-rl-for-robotics",children:"Introduction to RL for Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Reinforcement Learning (RL) has emerged as a powerful paradigm for training intelligent robotic behaviors, particularly for complex tasks that are difficult to program explicitly. Isaac Sim provides an ideal environment for developing and testing RL algorithms for humanoid robots, offering realistic physics simulation, photorealistic rendering, and seamless integration with popular RL frameworks."}),"\n",(0,s.jsx)(n.h2,{id:"rl-fundamentals-for-robotics",children:"RL Fundamentals for Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"markov-decision-processes-in-robotics",children:"Markov Decision Processes in Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Robotic tasks can be formulated as Markov Decision Processes (MDPs):"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"State Space (S)"}),": Robot's sensor readings, joint positions, velocities, and environmental observations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Space (A)"}),": Joint commands, Cartesian velocities, or high-level behaviors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reward Function (R)"}),": Scalar feedback based on task success, energy efficiency, or safety"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transition Dynamics (P)"}),": Physics simulation governing state transitions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Discount Factor (\u03b3)"}),": Trade-off between immediate and future rewards"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"rl-problem-formulation",children:"RL Problem Formulation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport numpy as np\r\nfrom omni.isaac.gym import IsaacEnv\r\nfrom omni.isaac.core.articulations import ArticulationView\r\n\r\nclass RobotRLTask(IsaacEnv):\r\n    def __init__(self, cfg):\r\n        super().__init__(cfg)\r\n\r\n        # Define action and observation spaces\r\n        self.action_space = self.define_action_space()\r\n        self.observation_space = self.define_observation_space()\r\n\r\n        # Initialize robot and environment\r\n        self.robot = ArticulationView(\r\n            prim_paths_expr="/World/Robot/.*",\r\n            name="robot_view"\r\n        )\r\n\r\n        self.task_params = cfg["task_params"]\r\n\r\n    def define_action_space(self):\r\n        # Define continuous action space for humanoid robot\r\n        action_dim = 12  # Number of joints to control\r\n        action_low = np.ones(action_dim) * -1.0\r\n        action_high = np.ones(action_dim) * 1.0\r\n        return gym.spaces.Box(action_low, action_high)\r\n\r\n    def define_observation_space(self):\r\n        # Define observation space including robot state and task info\r\n        obs_dim = 48  # Combined state dimensions\r\n        obs_low = np.ones(obs_dim) * -np.inf\r\n        obs_high = np.ones(obs_dim) * np.inf\r\n        return gym.spaces.Box(obs_low, obs_high)\r\n\r\n    def get_observations(self):\r\n        # Get current robot state and task-relevant information\r\n        robot_pos, robot_orn = self.robot.get_world_poses()\r\n        robot_vel = self.robot.get_velocities()\r\n        target_pos = self.get_target_position()\r\n\r\n        # Combine into observation vector\r\n        obs = np.concatenate([\r\n            robot_pos,\r\n            robot_orn,\r\n            robot_vel,\r\n            target_pos,\r\n            self.get_task_specific_info()\r\n        ])\r\n\r\n        return torch.from_numpy(obs).float()\r\n\r\n    def calculate_reward(self, actions):\r\n        # Calculate reward based on task progress\r\n        current_pos = self.robot.get_world_poses()[0]\r\n        target_pos = self.get_target_position()\r\n\r\n        # Distance-based reward\r\n        distance_to_target = torch.norm(target_pos - current_pos)\r\n        distance_reward = torch.exp(-distance_to_target)\r\n\r\n        # Bonus for reaching target\r\n        target_reached = distance_to_target < self.task_params["success_threshold"]\r\n        success_bonus = 10.0 if target_reached else 0.0\r\n\r\n        # Penalty for unsafe actions\r\n        action_penalty = torch.mean(torch.square(actions))\r\n\r\n        total_reward = distance_reward + success_bonus - 0.01 * action_penalty\r\n        return total_reward\n'})}),"\n",(0,s.jsx)(n.h2,{id:"isaac-sim-rl-integration",children:"Isaac Sim RL Integration"}),"\n",(0,s.jsx)(n.h3,{id:"isaac-gym-environment",children:"Isaac Gym Environment"}),"\n",(0,s.jsx)(n.p,{children:"Isaac Sim provides a specialized environment for RL training:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from omni.isaac.gym.vec_env import VecEnvGPU\r\nfrom omni.isaac.core.utils.torch.maths import torch\r\nimport torch.nn as nn\r\n\r\nclass IsaacVecEnvGPU(VecEnvGPU):\r\n    def __init__(self, task, sim_device, rl_device, graphics_device_id, headless):\r\n        super().__init__(\r\n            task=task,\r\n            sim_device=sim_device,\r\n            rl_device=rl_device,\r\n            graphics_device_id=graphics_device_id,\r\n            headless=headless\r\n        )\r\n\r\n    def reset(self):\r\n        # Reset all environments in the vectorized environment\r\n        super().reset()\r\n        return self.get_observations()\r\n\r\n    def step(self, actions):\r\n        # Execute actions and return observations, rewards, etc.\r\n        self.send_acctions(actions)\r\n        self.render()\r\n        obs = self.get_observations()\r\n        rewards = self.get_rewards()\r\n        dones = self.get_dones()\r\n        info = self.get_extras()\r\n\r\n        return obs, rewards, dones, info\n"})}),"\n",(0,s.jsx)(n.h3,{id:"physics-simulation-for-rl",children:"Physics Simulation for RL"}),"\n",(0,s.jsx)(n.p,{children:"Isaac Sim's PhysX engine provides realistic physics for RL training:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accurate contact simulation"}),": Realistic friction, collision, and contact handling"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rigid body dynamics"}),": Proper mass, inertia, and force calculations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-body systems"}),": Complex articulated robots with constraints"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Soft body simulation"}),": Deformable objects and environments"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"deep-rl-algorithms-for-robotics",children:"Deep RL Algorithms for Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"deep-deterministic-policy-gradient-ddpg",children:"Deep Deterministic Policy Gradient (DDPG)"}),"\n",(0,s.jsx)(n.p,{children:"DDPG is well-suited for continuous control tasks in robotics:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch.nn as nn\r\nimport torch.optim as optim\r\n\r\nclass DDPGActor(nn.Module):\r\n    def __init__(self, state_dim, action_dim, max_action):\r\n        super(DDPGActor, self).__init__()\r\n\r\n        self.l1 = nn.Linear(state_dim, 256)\r\n        self.l2 = nn.Linear(256, 256)\r\n        self.l3 = nn.Linear(256, action_dim)\r\n\r\n        self.max_action = max_action\r\n\r\n    def forward(self, state):\r\n        a = torch.relu(self.l1(state))\r\n        a = torch.relu(self.l2(a))\r\n        return self.max_action * torch.tanh(self.l3(a))\r\n\r\nclass DDPGCritic(nn.Module):\r\n    def __init__(self, state_dim, action_dim):\r\n        super(DDGCritic, self).__init__()\r\n\r\n        self.l1 = nn.Linear(state_dim + action_dim, 256)\r\n        self.l2 = nn.Linear(256, 256)\r\n        self.l3 = nn.Linear(256, 1)\r\n\r\n    def forward(self, state, action):\r\n        sa = torch.cat([state, action], 1)\r\n        q = torch.relu(self.l1(sa))\r\n        q = torch.relu(self.l2(q))\r\n        return self.l3(q)\r\n\r\nclass DDPGAgent:\r\n    def __init__(self, state_dim, action_dim, max_action):\r\n        self.actor = DDPGActor(state_dim, action_dim, max_action).to(device)\r\n        self.actor_target = DDPGActor(state_dim, action_dim, max_action).to(device)\r\n        self.critic = DDPGCritic(state_dim, action_dim).to(device)\r\n        self.critic_target = DDPGCritic(state_dim, action_dim).to(device)\r\n\r\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\r\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"soft-actor-critic-sac",children:"Soft Actor-Critic (SAC)"}),"\n",(0,s.jsx)(n.p,{children:"SAC provides better sample efficiency and stability:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class SACAgent:\r\n    def __init__(self, state_dim, action_dim, max_action):\r\n        self.actor = GaussianPolicy(state_dim, action_dim, max_action).to(device)\r\n        self.critic_1 = QNetwork(state_dim, action_dim).to(device)\r\n        self.critic_2 = QNetwork(state_dim, action_dim).to(device)\r\n\r\n        self.target_entropy = -torch.prod(torch.Tensor(action_dim)).item()\r\n        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\r\n        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=1e-4)\r\n\r\n    def select_action(self, state, evaluate=False):\r\n        state = torch.FloatTensor(state).to(device).unsqueeze(0)\r\n        if evaluate is False:\r\n            action, _, _ = self.actor.sample(state)\r\n        else:\r\n            _, _, action = self.actor.sample(state)\r\n        return action.cpu().data.numpy().flatten()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"proximal-policy-optimization-ppo",children:"Proximal Policy Optimization (PPO)"}),"\n",(0,s.jsx)(n.p,{children:"PPO is particularly stable for humanoid locomotion:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class PPOAgent:\r\n    def __init__(self, state_dim, action_dim):\r\n        self.actor = Actor(state_dim, action_dim).to(device)\r\n        self.critic = Critic(state_dim).to(device)\r\n\r\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\r\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=3e-4)\r\n\r\n        self.ppo_epochs = 10\r\n        self.clip_epsilon = 0.2\r\n\r\n    def update(self, states, actions, log_probs, returns, advantages):\r\n        for _ in range(self.ppo_epochs):\r\n            # Get current policy probabilities\r\n            dist = self.actor(states)\r\n            current_log_probs = dist.log_prob(actions)\r\n\r\n            # Calculate ratio\r\n            ratio = torch.exp(current_log_probs - log_probs)\r\n\r\n            # Calculate surrogate objectives\r\n            surr1 = ratio * advantages\r\n            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\r\n\r\n            # Actor loss\r\n            actor_loss = -torch.min(surr1, surr2).mean()\r\n\r\n            # Critic loss\r\n            current_values = self.critic(states)\r\n            critic_loss = F.mse_loss(current_values, returns)\r\n\r\n            # Update networks\r\n            self.actor_optimizer.zero_grad()\r\n            actor_loss.backward()\r\n            self.actor_optimizer.step()\r\n\r\n            self.critic_optimizer.zero_grad()\r\n            critic_loss.backward()\r\n            self.critic_optimizer.step()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"isaac-sim-rl-examples",children:"Isaac Sim RL Examples"}),"\n",(0,s.jsx)(n.h3,{id:"humanoid-locomotion",children:"Humanoid Locomotion"}),"\n",(0,s.jsx)(n.p,{children:"Training bipedal locomotion with RL:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class HumanoidLocomotionTask(RobotRLTask):\r\n    def __init__(self, cfg):\r\n        super().__init__(cfg)\r\n        self.target_velocity = cfg["target_velocity"]\r\n        self.max_episode_length = cfg["max_episode_length"]\r\n\r\n    def get_observations(self):\r\n        # Get humanoid-specific observations\r\n        base_pos, base_orn = self.robot.get_world_poses()\r\n        base_vel = self.robot.get_velocities()\r\n\r\n        # Extract humanoid-specific features\r\n        torso_pos = base_pos\r\n        torso_orn = base_orn\r\n        linear_vel = base_vel[:, :3]\r\n        angular_vel = base_vel[:, 3:6]\r\n\r\n        # Joint positions and velocities\r\n        joint_pos = self.robot.get_joint_positions()\r\n        joint_vel = self.robot.get_joint_velocities()\r\n\r\n        # Combine all observations\r\n        obs = torch.cat([\r\n            torso_pos,\r\n            torso_orn,\r\n            linear_vel,\r\n            angular_vel,\r\n            joint_pos,\r\n            joint_vel,\r\n            self.target_velocity\r\n        ], dim=-1)\r\n\r\n        return obs\r\n\r\n    def calculate_reward(self, actions):\r\n        # Reward for forward locomotion\r\n        base_pos, base_orn = self.robot.get_world_poses()\r\n        base_vel = self.robot.get_velocities()\r\n\r\n        # Forward velocity reward\r\n        forward_vel = base_vel[:, 0]  # x-axis velocity\r\n        forward_reward = torch.clamp(forward_vel, 0, None)\r\n\r\n        # Standing upright reward\r\n        target_orn = torch.tensor([0, 0, 0, 1]).to(self.device)  # upright quaternion\r\n        orientation_reward = 1.0 - torch.abs(torch.sum(base_orn * target_orn, dim=-1))\r\n\r\n        # Action smoothness penalty\r\n        action_penalty = torch.mean(torch.square(actions))\r\n\r\n        # Joint position limits penalty\r\n        joint_pos = self.robot.get_joint_positions()\r\n        joint_limit_penalty = torch.mean(torch.clamp(torch.abs(joint_pos) - 2.0, min=0.0))\r\n\r\n        total_reward = (\r\n            1.0 * forward_reward +\r\n            0.5 * orientation_reward -\r\n            0.01 * action_penalty -\r\n            0.1 * joint_limit_penalty\r\n        )\r\n\r\n        return total_reward\n'})}),"\n",(0,s.jsx)(n.h3,{id:"manipulation-tasks",children:"Manipulation Tasks"}),"\n",(0,s.jsx)(n.p,{children:"Training manipulation skills with RL:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class ManipulationTask(RobotRLTask):\r\n    def __init__(self, cfg):\r\n        super().__init__(cfg)\r\n        self.object_pos = None\r\n        self.target_pos = None\r\n\r\n    def reset(self):\r\n        # Reset object and target positions\r\n        self.object_pos = self.generate_random_object_pos()\r\n        self.target_pos = self.generate_random_target_pos()\r\n\r\n        # Place object and target in environment\r\n        self.place_object(self.object_pos)\r\n        self.place_target(self.target_pos)\r\n\r\n        return self.get_observations()\r\n\r\n    def get_observations(self):\r\n        # Get manipulation-specific observations\r\n        ee_pos, ee_orn = self.robot.get_end_effector_poses()\r\n        object_pos = self.get_object_position()\r\n        target_pos = self.target_pos\r\n\r\n        # Joint states\r\n        joint_pos = self.robot.get_joint_positions()\r\n        joint_vel = self.robot.get_joint_velocities()\r\n\r\n        obs = torch.cat([\r\n            ee_pos,           # End effector position\r\n            ee_orn,           # End effector orientation\r\n            object_pos,       # Object position\r\n            target_pos,       # Target position\r\n            joint_pos,        # Joint positions\r\n            joint_vel,        # Joint velocities\r\n            ee_pos - object_pos,  # Relative position to object\r\n            object_pos - target_pos  # Relative position to target\r\n        ], dim=-1)\r\n\r\n        return obs\r\n\r\n    def calculate_reward(self, actions):\r\n        ee_pos = self.robot.get_end_effector_positions()\r\n        object_pos = self.get_object_position()\r\n        target_pos = self.target_pos\r\n\r\n        # Grasping reward\r\n        grasp_distance = torch.norm(ee_pos - object_pos, dim=-1)\r\n        grasp_reward = torch.exp(-grasp_distance)\r\n\r\n        # Object-to-target reward\r\n        object_to_target = torch.norm(object_pos - target_pos, dim=-1)\r\n        placement_reward = torch.exp(-object_to_target)\r\n\r\n        # Bonus for successful placement\r\n        success = object_to_target < 0.1  # 10cm threshold\r\n        success_bonus = 10.0 * success.float()\r\n\r\n        total_reward = grasp_reward + placement_reward + success_bonus\r\n        return total_reward\n"})}),"\n",(0,s.jsx)(n.h2,{id:"sample-efficiency-and-transfer-learning",children:"Sample Efficiency and Transfer Learning"}),"\n",(0,s.jsx)(n.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,s.jsx)(n.p,{children:"Domain randomization improves sim-to-real transfer:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class DomainRandomization:\r\n    def __init__(self, env):\r\n        self.env = env\r\n        self.param_ranges = self.define_param_ranges()\r\n\r\n    def define_param_ranges(self):\r\n        return {\r\n            'robot_mass': [0.8, 1.2],  # 80% to 120% of nominal mass\r\n            'friction': [0.5, 1.5],    # Friction coefficient range\r\n            'restitution': [0.0, 0.2], # Bounciness range\r\n            'actuator_strength': [0.8, 1.2], # Actuator force range\r\n            'sensor_noise': [0.0, 0.05] # Sensor noise range\r\n        }\r\n\r\n    def randomize_environment(self):\r\n        # Randomize physics parameters\r\n        for param, (min_val, max_val) in self.param_ranges.items():\r\n            if param == 'robot_mass':\r\n                self.set_robot_mass(\r\n                    self.nominal_mass * np.random.uniform(min_val, max_val)\r\n                )\r\n            elif param == 'friction':\r\n                self.set_friction_coefficient(\r\n                    np.random.uniform(min_val, max_val)\r\n                )\r\n            # ... continue for other parameters\r\n\r\n    def train_with_randomization(self, agent, num_episodes):\r\n        for episode in range(num_episodes):\r\n            # Randomize environment at the start of each episode\r\n            if episode % 10 == 0:  # Randomize every 10 episodes\r\n                self.randomize_environment()\r\n\r\n            # Train agent in randomized environment\r\n            self.train_agent(agent)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"curriculum-learning",children:"Curriculum Learning"}),"\n",(0,s.jsx)(n.p,{children:"Gradually increase task difficulty:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class CurriculumLearning:\r\n    def __init__(self, tasks):\r\n        self.tasks = tasks\r\n        self.current_task = 0\r\n        self.performance_threshold = 0.8  # 80% success rate\r\n\r\n    def evaluate_performance(self, agent):\r\n        # Evaluate agent on current task\r\n        successes = 0\r\n        total_episodes = 100\r\n\r\n        for episode in range(total_episodes):\r\n            episode_reward = self.run_episode(agent)\r\n            if episode_reward >= self.tasks[self.current_task].success_threshold:\r\n                successes += 1\r\n\r\n        return successes / total_episodes\r\n\r\n    def advance_curriculum(self, agent):\r\n        current_performance = self.evaluate_performance(agent)\r\n\r\n        if current_performance >= self.performance_threshold:\r\n            if self.current_task < len(self.tasks) - 1:\r\n                self.current_task += 1\r\n                print(f"Advancing to task {self.current_task + 1}")\r\n\r\n                # Modify task parameters for increased difficulty\r\n                self.modify_task_difficulty()\r\n\r\n        return self.current_task\r\n\r\n    def modify_task_difficulty(self):\r\n        # Example: Increase target velocity for locomotion task\r\n        if self.current_task == 1:  # Walking\r\n            self.tasks[self.current_task].target_velocity *= 1.2\r\n        elif self.current_task == 2:  # Running\r\n            self.tasks[self.current_task].target_velocity *= 1.1\n'})}),"\n",(0,s.jsx)(n.h2,{id:"isaac-sim-rl-training-pipeline",children:"Isaac Sim RL Training Pipeline"}),"\n",(0,s.jsx)(n.h3,{id:"training-configuration",children:"Training Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# rl_training_config.yaml\r\nrl_games_config:\r\n  params:\r\n    seed: 42\r\n    algo:\r\n      name: a2c_continuous\r\n    model:\r\n      name: continuous_a2c_logstd\r\n    network:\r\n      name: actor_critic\r\n      separate: False\r\n      space:\r\n        continuous:\r\n          mu_activation: None\r\n          sigma_activation: None\r\n          mu_init:\r\n            name: default\r\n          sigma_init:\r\n            name: const_initializer\r\n            val: 0\r\n          fixed_sigma: True\r\n    train:\r\n      learning_rate: 3e-4\r\n      schedule: adaptive\r\n      kl_threshold: 0.008\r\n      resampling_freq: 4\r\n      max_episodes: 100000\r\n      save_best_after: 100\r\n      save_frequency: 500\r\n      grad_norm: 1.0\r\n      entropy_coef: 0.0\r\n      truncate_grads: True\r\n      e_clip: 0.2\r\n      horizon_length: 32\r\n      minibatch_size: 64\r\n      mini_epochs: 4\r\n      critic_coef: 2\r\n      clip_value: True\n"})}),"\n",(0,s.jsx)(n.h3,{id:"multi-environment-training",children:"Multi-Environment Training"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def train_rl_agent():\r\n    # Create multiple environments for parallel training\r\n    num_envs = 4096  # Large batch for efficient training\r\n    envs_per_instance = 128\r\n\r\n    # Initialize Isaac Sim environments\r\n    envs = []\r\n    for i in range(0, num_envs, envs_per_instance):\r\n        env = HumanoidLocomotionTask(\r\n            cfg={\r\n                'num_envs': min(envs_per_instance, num_envs - i),\r\n                'env_spacing': 2.0,\r\n                'episode_length': 1000,\r\n                'asset_root': '/path/to/assets',\r\n                'asset_file': 'humanoid.urdf'\r\n            }\r\n        )\r\n        envs.append(env)\r\n\r\n    # Initialize RL agent\r\n    agent = PPOAgent(\r\n        state_dim=envs[0].observation_space.shape[0],\r\n        action_dim=envs[0].action_space.shape[0]\r\n    )\r\n\r\n    # Training loop\r\n    for episode in range(100000):\r\n        # Collect experiences from all environments\r\n        experiences = collect_experiences(envs, agent)\r\n\r\n        # Update agent with collected experiences\r\n        agent.update(experiences)\r\n\r\n        # Log training metrics\r\n        if episode % 100 == 0:\r\n            log_metrics(episode, experiences, agent)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"safety-and-robustness",children:"Safety and Robustness"}),"\n",(0,s.jsx)(n.h3,{id:"safe-rl-in-isaac-sim",children:"Safe RL in Isaac Sim"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class SafeRLAgent:\r\n    def __init__(self, state_dim, action_dim):\r\n        self.nominal_agent = PPOAgent(state_dim, action_dim)\r\n        self.safety_critic = SafetyCritic(state_dim).to(device)\r\n        self.safety_threshold = 0.1  # Maximum risk threshold\r\n\r\n    def safe_action_selection(self, state):\r\n        # Get nominal action from policy\r\n        nominal_action = self.nominal_agent.select_action(state)\r\n\r\n        # Evaluate safety of action\r\n        safety_score = self.safety_critic.evaluate(state, nominal_action)\r\n\r\n        if safety_score > self.safety_threshold:\r\n            # Use safe fallback action\r\n            safe_action = self.get_safe_fallback_action(state)\r\n            return safe_action\r\n        else:\r\n            return nominal_action\r\n\r\n    def incorporate_safety_rewards(self, rewards, states, actions):\r\n        # Add safety penalties to rewards\r\n        safety_costs = self.safety_critic.get_safety_costs(states, actions)\r\n        safe_rewards = rewards - 10.0 * torch.clamp(safety_costs - self.safety_threshold, min=0.0)\r\n        return safe_rewards\n"})}),"\n",(0,s.jsx)(n.h3,{id:"robustness-testing",children:"Robustness Testing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class RobustnessEvaluator:\r\n    def __init__(self, trained_agent):\r\n        self.agent = trained_agent\r\n        self.disturbance_types = [\r\n            'external_force',\r\n            'sensor_noise',\r\n            'actuator_delay',\r\n            'model_uncertainty'\r\n        ]\r\n\r\n    def test_robustness(self):\r\n        robustness_results = {}\r\n\r\n        for disturbance_type in self.disturbance_types:\r\n            success_rate = self.apply_disturbance_and_test(\r\n                disturbance_type=disturbance_type,\r\n                magnitude_range=[0.1, 1.0]\r\n            )\r\n            robustness_results[disturbance_type] = success_rate\r\n\r\n        return robustness_results\r\n\r\n    def apply_disturbance_and_test(self, disturbance_type, magnitude_range):\r\n        # Apply disturbance and measure success rate\r\n        successes = 0\r\n        total_tests = 100\r\n\r\n        for test in range(total_tests):\r\n            magnitude = np.random.uniform(*magnitude_range)\r\n            success = self.test_single_disturbance(\r\n                disturbance_type, magnitude\r\n            )\r\n            if success:\r\n                successes += 1\r\n\r\n        return successes / total_tests\n"})}),"\n",(0,s.jsx)(n.h2,{id:"advanced-rl-techniques",children:"Advanced RL Techniques"}),"\n",(0,s.jsx)(n.h3,{id:"meta-learning-for-robotics",children:"Meta-Learning for Robotics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class MetaRLAgent:\r\n    def __init__(self, state_dim, action_dim):\r\n        self.meta_learner = MAML(state_dim, action_dim)\r\n        self.fast_adaptation_steps = 5\r\n\r\n    def adapt_to_new_task(self, new_task_data):\r\n        # Adapt quickly to new task using meta-learning\r\n        adapted_agent = self.meta_learner.adapt(\r\n            task_data=new_task_data,\r\n            num_steps=self.fast_adaptation_steps\r\n        )\r\n        return adapted_agent\r\n\r\n    def learn_to_adapt(self, tasks):\r\n        # Meta-training phase: learn to adapt quickly\r\n        for meta_batch in tasks:\r\n            for task in meta_batch:\r\n                # Sample trajectories\r\n                trajectories = self.sample_trajectories(task)\r\n\r\n                # Adapt to task\r\n                adapted_params = self.meta_learner.adapt(\r\n                    trajectories, task\r\n                )\r\n\r\n                # Evaluate on test set\r\n                test_reward = self.evaluate_on_task(\r\n                    adapted_params, task.test_set\r\n                )\r\n\r\n                # Update meta-learner\r\n                self.meta_learner.update(test_reward)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"multi-task-rl",children:"Multi-Task RL"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class MultiTaskRLAgent:\r\n    def __init__(self, tasks, state_dim, action_dim):\r\n        self.tasks = tasks\r\n        self.shared_encoder = SharedEncoder(state_dim)\r\n        self.task_specific_heads = nn.ModuleList([\r\n            TaskHead(action_dim) for _ in tasks\r\n        ])\r\n        self.task_classifier = TaskClassifier(len(tasks))\r\n\r\n    def forward(self, state, task_id=None):\r\n        # Encode shared features\r\n        shared_features = self.shared_encoder(state)\r\n\r\n        if task_id is not None:\r\n            # Use specific task head\r\n            action = self.task_specific_heads[task_id](shared_features)\r\n        else:\r\n            # Infer task and use appropriate head\r\n            task_probs = self.task_classifier(shared_features)\r\n            task_id = torch.argmax(task_probs)\r\n            action = self.task_specific_heads[task_id](shared_features)\r\n\r\n        return action\n"})}),"\n",(0,s.jsx)(n.p,{children:"Reinforcement learning with Isaac Sim provides a powerful framework for training complex humanoid robot behaviors, combining realistic physics simulation with efficient learning algorithms to develop robust and adaptive robotic systems."})]})}function _(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var t=r(6540);const s={},i=t.createContext(s);function a(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);