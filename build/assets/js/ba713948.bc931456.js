"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[772],{2075:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"module4-vla/speech-recognition","title":"Speech Recognition and Natural Language Understanding","description":"Introduction to Speech Recognition in Robotics","source":"@site/docs/module4-vla/speech-recognition.md","sourceDirName":"module4-vla","slug":"/module4-vla/speech-recognition","permalink":"/docs/module4-vla/speech-recognition","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"modules","previous":{"title":"Cognitive Planning with LLMs to ROS 2 Actions","permalink":"/docs/module4-vla/cognitive-planning"},"next":{"title":"Multi-modal Interaction: Speech, Gesture, and Vision","permalink":"/docs/module4-vla/multimodal-interaction"}}');var i=t(4848),s=t(8453);const o={},a="Speech Recognition and Natural Language Understanding",c={},l=[{value:"Introduction to Speech Recognition in Robotics",id:"introduction-to-speech-recognition-in-robotics",level:2},{value:"Speech Recognition Technologies",id:"speech-recognition-technologies",level:2},{value:"Automatic Speech Recognition (ASR) Systems",id:"automatic-speech-recognition-asr-systems",level:3},{value:"Modern ASR Approaches",id:"modern-asr-approaches",level:3},{value:"Connectionist Temporal Classification (CTC)",id:"connectionist-temporal-classification-ctc",level:4},{value:"Attention-Based Models",id:"attention-based-models",level:4},{value:"Transformer-Based Models",id:"transformer-based-models",level:4},{value:"Natural Language Understanding (NLU)",id:"natural-language-understanding-nlu",level:2},{value:"NLU Pipeline",id:"nlu-pipeline",level:3},{value:"Deep Learning NLU",id:"deep-learning-nlu",level:3},{value:"Robotics-Specific NLU Challenges",id:"robotics-specific-nlu-challenges",level:2},{value:"Domain Adaptation",id:"domain-adaptation",level:3},{value:"Handling Ambiguity and Uncertainty",id:"handling-ambiguity-and-uncertainty",level:3},{value:"Integration with Speech Recognition Systems",id:"integration-with-speech-recognition-systems",level:2},{value:"Streaming Speech Processing",id:"streaming-speech-processing",level:3},{value:"Context-Aware Understanding",id:"context-aware-understanding",level:2},{value:"Maintaining Conversation Context",id:"maintaining-conversation-context",level:3},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:2},{value:"Robust Error Management",id:"robust-error-management",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Efficient Processing Pipelines",id:"efficient-processing-pipelines",level:3},{value:"Integration with VLA Systems",id:"integration-with-vla-systems",level:2},{value:"Complete VLA Pipeline",id:"complete-vla-pipeline",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"speech-recognition-and-natural-language-understanding",children:"Speech Recognition and Natural Language Understanding"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction-to-speech-recognition-in-robotics",children:"Introduction to Speech Recognition in Robotics"}),"\n",(0,i.jsx)(n.p,{children:"Speech recognition is a fundamental component of Vision-Language-Action (VLA) systems in humanoid robotics. It enables robots to understand and respond to human commands through natural language, making human-robot interaction more intuitive and accessible. Modern speech recognition systems have evolved significantly, moving from rule-based approaches to deep learning models that can handle various accents, languages, and noisy environments."}),"\n",(0,i.jsx)(n.h2,{id:"speech-recognition-technologies",children:"Speech Recognition Technologies"}),"\n",(0,i.jsx)(n.h3,{id:"automatic-speech-recognition-asr-systems",children:"Automatic Speech Recognition (ASR) Systems"}),"\n",(0,i.jsx)(n.p,{children:"Automatic Speech Recognition (ASR) systems convert spoken language into text. The core components include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Acoustic Models"}),": Map audio signals to phonetic units"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Language Models"}),": Determine the most likely word sequences"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pronunciation Models"}),": Define how words are pronounced"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Decoder"}),": Combines all models to produce text output"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"modern-asr-approaches",children:"Modern ASR Approaches"}),"\n",(0,i.jsx)(n.h4,{id:"connectionist-temporal-classification-ctc",children:"Connectionist Temporal Classification (CTC)"}),"\n",(0,i.jsx)(n.p,{children:"CTC allows end-to-end training of ASR systems without explicit alignment between audio and text. It's particularly useful for streaming applications where the entire audio sequence isn't available at once."}),"\n",(0,i.jsx)(n.h4,{id:"attention-based-models",children:"Attention-Based Models"}),"\n",(0,i.jsx)(n.p,{children:"Attention mechanisms allow models to focus on relevant parts of the input when generating output, improving recognition accuracy for longer sequences and complex sentences."}),"\n",(0,i.jsx)(n.h4,{id:"transformer-based-models",children:"Transformer-Based Models"}),"\n",(0,i.jsx)(n.p,{children:"Transformer architectures, like those used in Whisper, have shown exceptional performance by processing audio in parallel and capturing long-range dependencies effectively."}),"\n",(0,i.jsx)(n.h2,{id:"natural-language-understanding-nlu",children:"Natural Language Understanding (NLU)"}),"\n",(0,i.jsx)(n.h3,{id:"nlu-pipeline",children:"NLU Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"Natural Language Understanding goes beyond simple speech-to-text conversion to extract meaning from spoken commands:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import re\r\nimport spacy\r\nfrom typing import Dict, List, Tuple\r\n\r\nclass NaturalLanguageUnderstanding:\r\n    def __init__(self):\r\n        # Load spaCy model for linguistic analysis\r\n        try:\r\n            self.nlp = spacy.load("en_core_web_sm")\r\n        except OSError:\r\n            print("Please install spaCy English model: python -m spacy download en_core_web_sm")\r\n            self.nlp = None\r\n\r\n        # Define robot command patterns\r\n        self.command_patterns = [\r\n            {\r\n                "pattern": r"(?:go|move|navigate|walk)\\s+(?:to|toward|towards)\\s+(.+)",\r\n                "intent": "navigate_to",\r\n                "entities": ["location"]\r\n            },\r\n            {\r\n                "pattern": r"(?:pick|grasp|take|grab|get)\\s+(?:up\\s+)?(.+)",\r\n                "intent": "pick_object",\r\n                "entities": ["object"]\r\n            },\r\n            {\r\n                "pattern": r"(?:place|put|set|drop)\\s+(?:down\\s+)?(.+)",\r\n                "intent": "place_object",\r\n                "entities": ["object"]\r\n            },\r\n            {\r\n                "pattern": r"(?:find|locate|look\\s+for)\\s+(.+)",\r\n                "intent": "find_object",\r\n                "entities": ["object"]\r\n            }\r\n        ]\r\n\r\n    def process_command(self, text: str) -> Dict:\r\n        """Process natural language command and extract intent and entities"""\r\n        # Clean and normalize text\r\n        cleaned_text = self.clean_text(text)\r\n\r\n        # Extract intent and entities\r\n        intent, entities = self.extract_intent_and_entities(cleaned_text)\r\n\r\n        # Parse semantic structure\r\n        semantic_structure = self.parse_semantic_structure(cleaned_text, intent, entities)\r\n\r\n        return {\r\n            "original_text": text,\r\n            "cleaned_text": cleaned_text,\r\n            "intent": intent,\r\n            "entities": entities,\r\n            "semantic_structure": semantic_structure,\r\n            "confidence": self.calculate_confidence(text, intent)\r\n        }\r\n\r\n    def clean_text(self, text: str) -> str:\r\n        """Clean and normalize input text"""\r\n        # Convert to lowercase\r\n        text = text.lower()\r\n\r\n        # Remove extra whitespace\r\n        text = re.sub(r\'\\s+\', \' \', text).strip()\r\n\r\n        # Remove common filler words\r\n        filler_words = [\'um\', \'uh\', \'like\', \'you know\', \'so\', \'well\']\r\n        for filler in filler_words:\r\n            text = re.sub(r\'\\b\' + filler + r\'\\b\', \'\', text)\r\n\r\n        # Remove punctuation (except for specific cases)\r\n        text = re.sub(r\'[^\\w\\s]\', \' \', text)\r\n\r\n        return text.strip()\r\n\r\n    def extract_intent_and_entities(self, text: str) -> Tuple[str, Dict]:\r\n        """Extract intent and entities using pattern matching"""\r\n        for pattern_config in self.command_patterns:\r\n            match = re.search(pattern_config["pattern"], text)\r\n            if match:\r\n                entities = {}\r\n                for i, entity_name in enumerate(pattern_config["entities"]):\r\n                    if i < len(match.groups()):\r\n                        entities[entity_name] = match.group(i + 1)\r\n\r\n                return pattern_config["intent"], entities\r\n\r\n        # If no pattern matches, return general intent\r\n        return "unknown", {}\r\n\r\n    def parse_semantic_structure(self, text: str, intent: str, entities: Dict) -> Dict:\r\n        """Parse the semantic structure of the command"""\r\n        if not self.nlp:\r\n            return {"intent": intent, "entities": entities}\r\n\r\n        doc = self.nlp(text)\r\n\r\n        # Extract additional linguistic features\r\n        semantic_features = {\r\n            "root_verb": None,\r\n            "noun_phrases": [],\r\n            "adjectives": [],\r\n            "prepositions": [],\r\n            "named_entities": []\r\n        }\r\n\r\n        # Find root verb\r\n        for token in doc:\r\n            if token.dep_ == "ROOT" and token.pos_ == "VERB":\r\n                semantic_features["root_verb"] = token.lemma_\r\n                break\r\n\r\n        # Extract noun phrases\r\n        for chunk in doc.noun_chunks:\r\n            semantic_features["noun_phrases"].append({\r\n                "text": chunk.text,\r\n                "root": chunk.root.text,\r\n                "dependencies": [token.text for token in chunk]\r\n            })\r\n\r\n        # Extract adjectives and prepositions\r\n        for token in doc:\r\n            if token.pos_ == "ADJ":\r\n                semantic_features["adjectives"].append(token.text)\r\n            elif token.pos_ == "ADP":\r\n                semantic_features["prepositions"].append(token.text)\r\n\r\n        # Extract named entities\r\n        for ent in doc.ents:\r\n            semantic_features["named_entities"].append({\r\n                "text": ent.text,\r\n                "label": ent.label_,\r\n                "description": spacy.explain(ent.label_)\r\n            })\r\n\r\n        return semantic_features\r\n\r\n    def calculate_confidence(self, original_text: str, intent: str) -> float:\r\n        """Calculate confidence score for the parsed command"""\r\n        if intent == "unknown":\r\n            return 0.1  # Low confidence for unknown intents\r\n\r\n        # Calculate based on various factors\r\n        confidence = 0.5  # Base confidence\r\n\r\n        # Length factor: longer, more specific commands are more confident\r\n        if len(original_text.split()) >= 3:\r\n            confidence += 0.2\r\n\r\n        # Pattern match factor: exact pattern matches get higher confidence\r\n        for pattern_config in self.command_patterns:\r\n            if re.search(pattern_config["pattern"], original_text.lower()):\r\n                confidence += 0.3\r\n                break\r\n\r\n        # Named entity factor: presence of named entities increases confidence\r\n        if self.nlp:\r\n            doc = self.nlp(original_text)\r\n            if len(doc.ents) > 0:\r\n                confidence += 0.1\r\n\r\n        return min(confidence, 1.0)  # Cap at 1.0\n'})}),"\n",(0,i.jsx)(n.h3,{id:"deep-learning-nlu",children:"Deep Learning NLU"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nfrom transformers import AutoTokenizer, AutoModel\r\n\r\nclass DeepNLUModel(nn.Module):\r\n    def __init__(self, model_name="bert-base-uncased", num_intents=10):\r\n        super().__init__()\r\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n        self.bert = AutoModel.from_pretrained(model_name)\r\n\r\n        # Intent classification head\r\n        self.intent_classifier = nn.Linear(self.bert.config.hidden_size, num_intents)\r\n\r\n        # Entity recognition head\r\n        self.entity_classifier = nn.Linear(self.bert.config.hidden_size, 5)  # B, I, L, U, O tags\r\n\r\n        self.dropout = nn.Dropout(0.1)\r\n\r\n    def forward(self, input_ids, attention_mask):\r\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\r\n        sequence_output = outputs.last_hidden_state\r\n        pooled_output = outputs.pooler_output\r\n\r\n        # Intent classification\r\n        intent_logits = self.intent_classifier(self.dropout(pooled_output))\r\n\r\n        # Entity recognition\r\n        entity_logits = self.entity_classifier(self.dropout(sequence_output))\r\n\r\n        return intent_logits, entity_logits\r\n\r\n    def predict(self, text: str):\r\n        """Predict intent and entities for input text"""\r\n        inputs = self.tokenizer(\r\n            text,\r\n            return_tensors="pt",\r\n            padding=True,\r\n            truncation=True,\r\n            max_length=512\r\n        )\r\n\r\n        with torch.no_grad():\r\n            intent_logits, entity_logits = self(\r\n                inputs["input_ids"],\r\n                inputs["attention_mask"]\r\n            )\r\n\r\n        # Get predictions\r\n        intent_pred = torch.argmax(intent_logits, dim=1).item()\r\n        entity_preds = torch.argmax(entity_logits, dim=2).squeeze().tolist()\r\n\r\n        return {\r\n            "intent": intent_pred,\r\n            "entities": entity_preds,\r\n            "intent_scores": torch.softmax(intent_logits, dim=1).squeeze().tolist()\r\n        }\n'})}),"\n",(0,i.jsx)(n.h2,{id:"robotics-specific-nlu-challenges",children:"Robotics-Specific NLU Challenges"}),"\n",(0,i.jsx)(n.h3,{id:"domain-adaptation",children:"Domain Adaptation"}),"\n",(0,i.jsx)(n.p,{children:"Robotic commands have specific characteristics that differ from general language:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class RoboticsNLU(NaturalLanguageUnderstanding):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # Robot-specific vocabulary and patterns\r\n        self.robot_vocabulary = {\r\n            "navigation": ["forward", "backward", "left", "right", "turn", "rotate", "move", "go"],\r\n            "manipulation": ["pick", "place", "grasp", "release", "lift", "put", "take"],\r\n            "objects": ["cup", "bottle", "box", "book", "chair", "table", "door", "window"],\r\n            "locations": ["kitchen", "living room", "bedroom", "office", "hallway", "bathroom"]\r\n        }\r\n\r\n        # Robot-specific command patterns\r\n        self.robot_patterns = [\r\n            # Relative positioning\r\n            r"(?:to the|the|near the|by the)\\s+(left|right|front|back|side)\\s+of\\s+(.+)",\r\n            # Quantifiers\r\n            r"(?:all|both|each|every)\\s+(.+)",\r\n            # Temporal aspects\r\n            r"(?:while|as|when|after|before)\\s+(.+)",\r\n        ]\r\n\r\n    def adapt_to_robot_domain(self, text: str) -> Dict:\r\n        """Adapt general NLU to robot-specific domain"""\r\n        result = self.process_command(text)\r\n\r\n        # Enhance with robot-specific understanding\r\n        result["robot_context"] = self.extract_robot_context(text)\r\n        result["spatial_relations"] = self.extract_spatial_relations(text)\r\n        result["temporal_constraints"] = self.extract_temporal_constraints(text)\r\n\r\n        return result\r\n\r\n    def extract_robot_context(self, text: str) -> Dict:\r\n        """Extract robot-specific context from text"""\r\n        context = {\r\n            "navigation": [],\r\n            "manipulation": [],\r\n            "objects": [],\r\n            "locations": []\r\n        }\r\n\r\n        for category, words in self.robot_vocabulary.items():\r\n            found_words = [word for word in words if word.lower() in text.lower()]\r\n            context[category] = found_words\r\n\r\n        return context\r\n\r\n    def extract_spatial_relations(self, text: str) -> List[Dict]:\r\n        """Extract spatial relationships from text"""\r\n        spatial_relations = []\r\n\r\n        # Look for spatial prepositions and their objects\r\n        spatial_patterns = [\r\n            r"(?:to the|on the|in the|at the|by the|next to the|near the|beside the|in front of the|behind the)\\s+(.+?)(?:\\s|$)",\r\n            r"(?:left of|right of|in front of|behind|above|below|next to|near|beside)\\s+(.+?)(?:\\s|$)"\r\n        ]\r\n\r\n        for pattern in spatial_patterns:\r\n            matches = re.findall(pattern, text, re.IGNORECASE)\r\n            for match in matches:\r\n                spatial_relations.append({\r\n                    "relation": pattern,\r\n                    "object": match.strip()\r\n                })\r\n\r\n        return spatial_relations\r\n\r\n    def extract_temporal_constraints(self, text: str) -> List[Dict]:\r\n        """Extract temporal constraints from text"""\r\n        temporal_indicators = [\r\n            (r"(?:while|during|as long as)", "concurrent"),\r\n            (r"(?:after|then|followed by)", "sequence"),\r\n            (r"(?:before|until|stop when)", "condition"),\r\n            (r"(?:for|during|over the next)", "duration")\r\n        ]\r\n\r\n        temporal_constraints = []\r\n        for pattern, constraint_type in temporal_indicators:\r\n            if re.search(pattern, text, re.IGNORECASE):\r\n                temporal_constraints.append({\r\n                    "pattern": pattern,\r\n                    "type": constraint_type,\r\n                    "text": text\r\n                })\r\n\r\n        return temporal_constraints\n'})}),"\n",(0,i.jsx)(n.h3,{id:"handling-ambiguity-and-uncertainty",children:"Handling Ambiguity and Uncertainty"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class RobustNLU(RoboticsNLU):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.ambiguity_threshold = 0.3\r\n\r\n    def handle_ambiguous_input(self, text: str) -> Dict:\r\n        """Handle ambiguous or uncertain input"""\r\n        # Multiple interpretations possible\r\n        interpretations = self.generate_interpretations(text)\r\n\r\n        if len(interpretations) > 1:\r\n            # Calculate ambiguity score\r\n            ambiguity_score = self.calculate_ambiguity_score(interpretations)\r\n\r\n            if ambiguity_score > self.ambiguity_threshold:\r\n                # Request clarification\r\n                clarification_request = self.generate_clarification_request(text, interpretations)\r\n                return {\r\n                    "status": "ambiguous",\r\n                    "interpretations": interpretations,\r\n                    "clarification_request": clarification_request,\r\n                    "confidence": ambiguity_score\r\n                }\r\n\r\n        # Process with highest confidence interpretation\r\n        return self.process_command(text)\r\n\r\n    def generate_interpretations(self, text: str) -> List[Dict]:\r\n        """Generate multiple possible interpretations of ambiguous text"""\r\n        interpretations = []\r\n\r\n        # Try different parsing strategies\r\n        basic_parse = self.process_command(text)\r\n        interpretations.append(basic_parse)\r\n\r\n        # Try with different entity groupings\r\n        entity_variations = self.generate_entity_variations(text)\r\n        for variation in entity_variations:\r\n            interpretations.append(variation)\r\n\r\n        # Try with different spatial interpretations\r\n        spatial_variations = self.generate_spatial_variations(text)\r\n        for variation in spatial_variations:\r\n            interpretations.append(variation)\r\n\r\n        return interpretations\r\n\r\n    def generate_entity_variations(self, text: str) -> List[Dict]:\r\n        """Generate variations based on different entity groupings"""\r\n        variations = []\r\n\r\n        # Example: "pick up the red cup on the table"\r\n        # Could be: pick (red cup) on the table\r\n        # Or: pick (red) cup on the table (if red is not an object property)\r\n\r\n        # This would involve more sophisticated parsing\r\n        # For now, we\'ll show the concept\r\n        return variations\r\n\r\n    def generate_clarification_request(self, text: str, interpretations: List[Dict]) -> str:\r\n        """Generate a clarification request for ambiguous input"""\r\n        if len(interpretations) == 0:\r\n            return "I didn\'t understand that command. Could you please rephrase it?"\r\n\r\n        # Find the most different interpretations\r\n        if len(interpretations) >= 2:\r\n            first_intent = interpretations[0].get("intent", "unknown")\r\n            second_intent = interpretations[1].get("intent", "unknown")\r\n\r\n            if first_intent != second_intent:\r\n                return f"I heard \'{text}\'. Did you mean to {first_intent} or {second_intent}?"\r\n\r\n        # Default clarification\r\n        return f"I\'m not sure what you mean by \'{text}\'. Could you please be more specific?"\n'})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-speech-recognition-systems",children:"Integration with Speech Recognition Systems"}),"\n",(0,i.jsx)(n.h3,{id:"streaming-speech-processing",children:"Streaming Speech Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import asyncio\r\nimport queue\r\nfrom dataclasses import dataclass\r\nfrom typing import Optional\r\n\r\n@dataclass\r\nclass SpeechSegment:\r\n    """Represents a segment of speech with timing and confidence"""\r\n    text: str\r\n    start_time: float\r\n    end_time: float\r\n    confidence: float\r\n    is_complete: bool\r\n\r\nclass StreamingSpeechProcessor:\r\n    def __init__(self, nlu_system, asr_system):\r\n        self.nlu_system = nlu_system\r\n        self.asr_system = asr_system\r\n        self.audio_buffer = queue.Queue()\r\n        self.speech_segments = []\r\n        self.current_sentence = ""\r\n        self.sentence_confidence = 0.0\r\n\r\n    async def process_streaming_audio(self, audio_stream):\r\n        """Process streaming audio and extract meaningful commands"""\r\n        async for audio_chunk in audio_stream:\r\n            # Process audio chunk with ASR\r\n            asr_result = await self.asr_system.process_chunk(audio_chunk)\r\n\r\n            if asr_result.is_final:\r\n                # Process complete segment\r\n                segment = SpeechSegment(\r\n                    text=asr_result.text,\r\n                    start_time=asr_result.start_time,\r\n                    end_time=asr_result.end_time,\r\n                    confidence=asr_result.confidence,\r\n                    is_complete=True\r\n                )\r\n\r\n                await self.process_complete_segment(segment)\r\n            else:\r\n                # Update partial result\r\n                self.current_sentence = asr_result.text\r\n                self.sentence_confidence = asr_result.confidence\r\n\r\n    async def process_complete_segment(self, segment: SpeechSegment):\r\n        """Process a complete speech segment with NLU"""\r\n        if segment.confidence > 0.7:  # Confidence threshold\r\n            # Process with NLU\r\n            nlu_result = self.nlu_system.process_command(segment.text)\r\n\r\n            # Check if it\'s a robot command\r\n            if self.is_robot_command(nlu_result):\r\n                # Execute robot action\r\n                await self.execute_robot_command(nlu_result)\r\n            else:\r\n                # Store for context or ignore\r\n                self.store_context(segment.text)\r\n\r\n    def is_robot_command(self, nlu_result: Dict) -> bool:\r\n        """Determine if the processed text is a robot command"""\r\n        robot_intents = ["navigate_to", "pick_object", "place_object", "find_object", "stop", "go", "help"]\r\n        return nlu_result.get("intent") in robot_intents\r\n\r\n    async def execute_robot_command(self, nlu_result: Dict):\r\n        """Execute robot command based on NLU result"""\r\n        # This would integrate with ROS 2 action system\r\n        print(f"Executing robot command: {nlu_result}")\r\n        # Implementation would send command to robot\n'})}),"\n",(0,i.jsx)(n.h2,{id:"context-aware-understanding",children:"Context-Aware Understanding"}),"\n",(0,i.jsx)(n.h3,{id:"maintaining-conversation-context",children:"Maintaining Conversation Context"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class ContextualNLU(RobustNLU):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conversation_context = {\r\n            "current_topic": None,\r\n            "recent_commands": [],\r\n            "robot_state": {},\r\n            "user_preferences": {},\r\n            "world_state": {}\r\n        }\r\n        self.max_context_length = 10\r\n\r\n    def update_context(self, command_result: Dict, execution_result: Dict = None):\r\n        """Update conversation context with new information"""\r\n        self.conversation_context["recent_commands"].append({\r\n            "command": command_result,\r\n            "execution_result": execution_result,\r\n            "timestamp": time.time()\r\n        })\r\n\r\n        # Maintain context window\r\n        if len(self.conversation_context["recent_commands"]) > self.max_context_length:\r\n            self.conversation_context["recent_commands"] = \\\r\n                self.conversation_context["recent_commands"][-self.max_context_length:]\r\n\r\n    def process_command_with_context(self, text: str) -> Dict:\r\n        """Process command using conversation context"""\r\n        # Apply context to disambiguate commands\r\n        contextual_text = self.apply_context_to_text(text)\r\n\r\n        # Process with enhanced context\r\n        result = self.process_command(contextual_text)\r\n\r\n        # Add context to result\r\n        result["context"] = self.conversation_context.copy()\r\n\r\n        return result\r\n\r\n    def apply_context_to_text(self, text: str) -> str:\r\n        """Apply conversation context to ambiguous text"""\r\n        # Example: "it" might refer to the last mentioned object\r\n        if "it" in text.lower():\r\n            last_object = self.get_last_mentioned_object()\r\n            if last_object:\r\n                # Replace "it" with the actual object name\r\n                text = re.sub(r\'\\bit\\b\', last_object, text, flags=re.IGNORECASE)\r\n\r\n        # Example: "there" might refer to the last mentioned location\r\n        if "there" in text.lower():\r\n            last_location = self.get_last_mentioned_location()\r\n            if last_location:\r\n                text = re.sub(r\'\\bthere\\b\', f"to {last_location}", text, flags=re.IGNORECASE)\r\n\r\n        return text\r\n\r\n    def get_last_mentioned_object(self) -> Optional[str]:\r\n        """Get the last mentioned object from conversation context"""\r\n        for command in reversed(self.conversation_context["recent_commands"]):\r\n            entities = command["command"].get("entities", {})\r\n            if "object" in entities:\r\n                return entities["object"]\r\n        return None\r\n\r\n    def get_last_mentioned_location(self) -> Optional[str]:\r\n        """Get the last mentioned location from conversation context"""\r\n        for command in reversed(self.conversation_context["recent_commands"]):\r\n            entities = command["command"].get("entities", {})\r\n            if "location" in entities:\r\n                return entities["location"]\r\n        return None\n'})}),"\n",(0,i.jsx)(n.h2,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,i.jsx)(n.h3,{id:"robust-error-management",children:"Robust Error Management"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class ErrorResilientNLU(ContextualNLU):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.error_recovery_strategies = [\r\n            self.retry_with_alternative_parsing,\r\n            self.request_clarification,\r\n            self.use_default_fallback\r\n        ]\r\n\r\n    def process_with_error_recovery(self, text: str) -> Dict:\r\n        """Process text with error recovery mechanisms"""\r\n        try:\r\n            # Primary processing\r\n            result = self.process_command_with_context(text)\r\n\r\n            # Validate result quality\r\n            if self.is_result_sufficient(result):\r\n                return result\r\n            else:\r\n                # Apply error recovery\r\n                return self.apply_error_recovery(text, result)\r\n\r\n        except Exception as e:\r\n            print(f"Error processing command: {e}")\r\n            return self.handle_processing_error(text, e)\r\n\r\n    def is_result_sufficient(self, result: Dict) -> bool:\r\n        """Check if the processing result is sufficient"""\r\n        confidence = result.get("confidence", 0.0)\r\n        intent = result.get("intent", "unknown")\r\n\r\n        # Sufficient if high confidence and known intent\r\n        if confidence > 0.8 and intent != "unknown":\r\n            return True\r\n\r\n        # Check if entities were extracted\r\n        entities = result.get("entities", {})\r\n        if len(entities) > 0 and confidence > 0.5:\r\n            return True\r\n\r\n        return False\r\n\r\n    def apply_error_recovery(self, text: str, initial_result: Dict) -> Dict:\r\n        """Apply error recovery strategies"""\r\n        for strategy in self.error_recovery_strategies:\r\n            try:\r\n                recovery_result = strategy(text, initial_result)\r\n                if recovery_result and self.is_result_sufficient(recovery_result):\r\n                    return recovery_result\r\n            except Exception as e:\r\n                print(f"Error in recovery strategy {strategy.__name__}: {e}")\r\n                continue\r\n\r\n        # If all strategies fail, return the best available result\r\n        return initial_result\r\n\r\n    def retry_with_alternative_parsing(self, text: str, initial_result: Dict) -> Optional[Dict]:\r\n        """Try alternative parsing approaches"""\r\n        # Try with different preprocessing\r\n        alternative_text = self.preprocess_alternative(text)\r\n        alternative_result = self.process_command(alternative_text)\r\n\r\n        # Compare with initial result\r\n        if alternative_result.get("confidence", 0) > initial_result.get("confidence", 0):\r\n            return alternative_result\r\n\r\n        return initial_result\r\n\r\n    def preprocess_alternative(self, text: str) -> str:\r\n        """Alternative preprocessing for error recovery"""\r\n        # Try removing potential noise words\r\n        noise_words = ["please", "could you", "can you", "would you"]\r\n        processed_text = text.lower()\r\n\r\n        for word in noise_words:\r\n            processed_text = processed_text.replace(word, "")\r\n\r\n        return processed_text.strip()\r\n\r\n    def request_clarification(self, text: str, initial_result: Dict) -> Optional[Dict]:\r\n        """Generate clarification request"""\r\n        return {\r\n            "status": "clarification_needed",\r\n            "original_text": text,\r\n            "suggested_interpretation": initial_result,\r\n            "clarification_request": f"I\'m not sure what you mean by \'{text}\'. Could you please rephrase?"\r\n        }\r\n\r\n    def use_default_fallback(self, text: str, initial_result: Dict) -> Optional[Dict]:\r\n        """Use default fallback response"""\r\n        return {\r\n            "intent": "unknown",\r\n            "entities": {},\r\n            "confidence": 0.1,\r\n            "fallback": True,\r\n            "message": "I didn\'t understand that command."\r\n        }\r\n\r\n    def handle_processing_error(self, text: str, error: Exception) -> Dict:\r\n        """Handle processing errors gracefully"""\r\n        return {\r\n            "status": "error",\r\n            "original_text": text,\r\n            "error": str(error),\r\n            "message": "Sorry, I encountered an error processing your command.",\r\n            "confidence": 0.0\r\n        }\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"efficient-processing-pipelines",children:"Efficient Processing Pipelines"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import time\r\nfrom functools import wraps\r\n\r\ndef timing_decorator(func):\r\n    """Decorator to measure function execution time"""\r\n    @wraps(func)\r\n    def wrapper(*args, **kwargs):\r\n        start_time = time.time()\r\n        result = func(*args, **kwargs)\r\n        end_time = time.time()\r\n        print(f"{func.__name__} took {end_time - start_time:.4f} seconds")\r\n        return result\r\n    return wrapper\r\n\r\nclass OptimizedNLU(ErrorResilientNLU):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.cached_results = {}\r\n        self.max_cache_size = 100\r\n\r\n    @timing_decorator\r\n    def process_command_with_cache(self, text: str) -> Dict:\r\n        """Process command with caching for performance"""\r\n        # Create cache key\r\n        cache_key = hash(text.lower().strip())\r\n\r\n        # Check cache first\r\n        if cache_key in self.cached_results:\r\n            print("Using cached result")\r\n            return self.cached_results[cache_key]\r\n\r\n        # Process command\r\n        result = self.process_with_error_recovery(text)\r\n\r\n        # Cache result\r\n        if len(self.cached_results) < self.max_cache_size:\r\n            self.cached_results[cache_key] = result\r\n\r\n        return result\r\n\r\n    def batch_process(self, texts: List[str]) -> List[Dict]:\r\n        """Process multiple texts efficiently"""\r\n        results = []\r\n\r\n        # Process in batches to optimize resource usage\r\n        batch_size = 10\r\n        for i in range(0, len(texts), batch_size):\r\n            batch = texts[i:i + batch_size]\r\n\r\n            # Process batch with shared resources\r\n            batch_results = []\r\n            for text in batch:\r\n                result = self.process_command_with_cache(text)\r\n                batch_results.append(result)\r\n\r\n            results.extend(batch_results)\r\n\r\n        return results\n'})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-vla-systems",children:"Integration with VLA Systems"}),"\n",(0,i.jsx)(n.h3,{id:"complete-vla-pipeline",children:"Complete VLA Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class VLAPipeline:\r\n    def __init__(self):\r\n        self.speech_recognizer = self.initialize_speech_recognizer()\r\n        self.nlu_system = OptimizedNLU()\r\n        self.action_generator = self.initialize_action_generator()\r\n        self.robot_interface = self.initialize_robot_interface()\r\n\r\n    def initialize_speech_recognizer(self):\r\n        """Initialize speech recognition system"""\r\n        # This would typically be Whisper or similar\r\n        return None  # Placeholder\r\n\r\n    def initialize_action_generator(self):\r\n        """Initialize action generation system"""\r\n        # This would connect to LLM-based action planning\r\n        return None  # Placeholder\r\n\r\n    def initialize_robot_interface(self):\r\n        """Initialize robot interface"""\r\n        # This would connect to ROS 2 system\r\n        return None  # Placeholder\r\n\r\n    def process_vla_request(self, audio_input) -> Dict:\r\n        """Complete VLA processing pipeline"""\r\n        try:\r\n            # Step 1: Speech Recognition\r\n            text = self.speech_recognizer.transcribe(audio_input)\r\n\r\n            # Step 2: Natural Language Understanding\r\n            nlu_result = self.nlu_system.process_command_with_cache(text)\r\n\r\n            # Step 3: Action Generation (Vision-Language integration would happen here)\r\n            action_plan = self.action_generator.generate_from_nlu(nlu_result)\r\n\r\n            # Step 4: Action Execution\r\n            execution_result = self.robot_interface.execute_action_plan(action_plan)\r\n\r\n            return {\r\n                "success": True,\r\n                "transcription": text,\r\n                "nlu_result": nlu_result,\r\n                "action_plan": action_plan,\r\n                "execution_result": execution_result,\r\n                "timestamp": time.time()\r\n            }\r\n\r\n        except Exception as e:\r\n            return {\r\n                "success": False,\r\n                "error": str(e),\r\n                "timestamp": time.time()\r\n            }\r\n\r\n    def continuous_listening_mode(self):\r\n        """Run VLA system in continuous listening mode"""\r\n        print("Starting continuous VLA processing...")\r\n\r\n        while True:\r\n            try:\r\n                # Capture audio from microphone or other source\r\n                audio_input = self.capture_audio()\r\n\r\n                if audio_input:\r\n                    result = self.process_vla_request(audio_input)\r\n\r\n                    if result["success"]:\r\n                        print(f"Command processed successfully: {result[\'transcription\']}")\r\n                    else:\r\n                        print(f"Command processing failed: {result[\'error\']}")\r\n\r\n            except KeyboardInterrupt:\r\n                print("Stopping VLA system...")\r\n                break\r\n            except Exception as e:\r\n                print(f"Error in continuous mode: {e}")\r\n                time.sleep(1)  # Brief pause before continuing\n'})}),"\n",(0,i.jsx)(n.p,{children:"Speech recognition and natural language understanding form the foundation of intuitive human-robot interaction, enabling robots to comprehend and respond to natural language commands while handling the complexities of real-world communication."})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var r=t(6540);const i={},s=r.createContext(i);function o(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);