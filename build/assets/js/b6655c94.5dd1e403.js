"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[691],{6072:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>_,frontMatter:()=>s,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module3-isaac/perception-manipulation","title":"Perception and Manipulation with Isaac Sim","description":"Introduction to Perception-Action Integration","source":"@site/docs/module3-isaac/perception-manipulation.md","sourceDirName":"module3-isaac","slug":"/module3-isaac/perception-manipulation","permalink":"/docs/module3-isaac/perception-manipulation","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"modules","previous":{"title":"Nav2 Path Planning for Bipedal Humanoids","permalink":"/docs/module3-isaac/nav2-planning"},"next":{"title":"Reinforcement Learning in Robotics with Isaac Sim","permalink":"/docs/module3-isaac/reinforcement-learning"}}');var t=r(4848),i=r(8453);const s={},o="Perception and Manipulation with Isaac Sim",l={},c=[{value:"Introduction to Perception-Action Integration",id:"introduction-to-perception-action-integration",level:2},{value:"Perception for Manipulation",id:"perception-for-manipulation",level:2},{value:"Object Detection and Recognition",id:"object-detection-and-recognition",level:3},{value:"6D Pose Estimation",id:"6d-pose-estimation",level:3},{value:"Grasp Point Detection",id:"grasp-point-detection",level:3},{value:"Manipulation Planning",id:"manipulation-planning",level:2},{value:"Motion Planning for Humanoid Arms",id:"motion-planning-for-humanoid-arms",level:3},{value:"Grasp Planning",id:"grasp-planning",level:3},{value:"Isaac Sim Manipulation Tools",id:"isaac-sim-manipulation-tools",level:2},{value:"Articulation View",id:"articulation-view",level:3},{value:"Contact and Grasp Simulation",id:"contact-and-grasp-simulation",level:3},{value:"Integration with Isaac ROS",id:"integration-with-isaac-ros",level:2},{value:"ROS Bridge for Manipulation",id:"ros-bridge-for-manipulation",level:3},{value:"Perception Pipeline Integration",id:"perception-pipeline-integration",level:3},{value:"Synthetic Data for Manipulation Learning",id:"synthetic-data-for-manipulation-learning",level:2},{value:"Dataset Generation",id:"dataset-generation",level:3},{value:"Domain Randomization for Manipulation",id:"domain-randomization-for-manipulation",level:3},{value:"Reinforcement Learning for Manipulation",id:"reinforcement-learning-for-manipulation",level:2},{value:"Isaac Sim RL Environment",id:"isaac-sim-rl-environment",level:3},{value:"Safety and Robustness",id:"safety-and-robustness",level:2},{value:"Collision Avoidance",id:"collision-avoidance",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Efficient Simulation",id:"efficient-simulation",level:3},{value:"Real-World Transfer",id:"real-world-transfer",level:2},{value:"Sim-to-Real Considerations",id:"sim-to-real-considerations",level:3}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"perception-and-manipulation-with-isaac-sim",children:"Perception and Manipulation with Isaac Sim"})}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-perception-action-integration",children:"Introduction to Perception-Action Integration"}),"\n",(0,t.jsx)(n.p,{children:"Perception and manipulation form the foundation of intelligent humanoid robot behavior. In the Isaac Sim environment, these capabilities are enhanced through photorealistic simulation, synthetic data generation, and hardware-accelerated processing, enabling the development of sophisticated manipulation skills that can transfer to real-world robots."}),"\n",(0,t.jsx)(n.h2,{id:"perception-for-manipulation",children:"Perception for Manipulation"}),"\n",(0,t.jsx)(n.h3,{id:"object-detection-and-recognition",children:"Object Detection and Recognition"}),"\n",(0,t.jsx)(n.p,{children:"Isaac Sim provides advanced object detection capabilities for manipulation tasks:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Example object detection for manipulation\r\nimport omni\r\nfrom omni.isaac.core.utils.prims import get_prim_at_path\r\nfrom omni.isaac.core.objects import cuboid\r\nfrom omni.isaac.sensor import Camera\r\nfrom omni.isaac.warehouse import Warehouse\r\n\r\nclass ManipulationPerception:\r\n    def __init__(self):\r\n        self.camera = Camera(\r\n            prim_path=\"/World/Robot/Camera\",\r\n            frequency=30,\r\n            resolution=(640, 480)\r\n        )\r\n\r\n        self.object_detector = self.setup_object_detector()\r\n\r\n    def setup_object_detector(self):\r\n        # Configure Isaac Sim's perception system\r\n        detector = {\r\n            'type': 'detection',\r\n            'model': 'Isaac-MonsterBot-Detect-Shelf',\r\n            'confidence_threshold': 0.7\r\n        }\r\n        return detector\r\n\r\n    def detect_objects_for_manipulation(self):\r\n        # Capture image and detect objects\r\n        rgb_image = self.camera.get_rgb()\r\n        depth_image = self.camera.get_depth()\r\n\r\n        # Process with Isaac Sim's perception pipeline\r\n        detections = self.run_object_detection(rgb_image)\r\n\r\n        # Filter for manipulable objects\r\n        manipulable_objects = self.filter_manipulable_objects(detections, depth_image)\r\n\r\n        return manipulable_objects\n"})}),"\n",(0,t.jsx)(n.h3,{id:"6d-pose-estimation",children:"6D Pose Estimation"}),"\n",(0,t.jsx)(n.p,{children:"Accurate pose estimation is crucial for manipulation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class PoseEstimator:\r\n    def __init__(self):\r\n        self.knowledge_base = {}  # Object models and properties\r\n\r\n    def estimate_6d_pose(self, object_id, rgb_image, depth_image):\r\n        # Use Isaac Sim's pose estimation capabilities\r\n        pose = self.isaac_pose_estimator.estimate(\r\n            object_id=object_id,\r\n            rgb_image=rgb_image,\r\n            depth_image=depth_image,\r\n            camera_intrinsics=self.get_camera_intrinsics()\r\n        )\r\n\r\n        return {\r\n            'position': pose.position,\r\n            'orientation': pose.orientation,\r\n            'confidence': pose.confidence\r\n        }\n"})}),"\n",(0,t.jsx)(n.h3,{id:"grasp-point-detection",children:"Grasp Point Detection"}),"\n",(0,t.jsx)(n.p,{children:"Identify optimal grasp points for manipulation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class GraspDetector:\r\n    def __init__(self):\r\n        self.grasp_network = self.load_grasp_network()\r\n\r\n    def detect_grasp_points(self, object_info):\r\n        # Use Isaac Sim's grasp detection\r\n        grasp_points = self.grasp_network.predict(\r\n            object_mesh=object_info['mesh'],\r\n            object_properties=object_info['properties'],\r\n            scene_context=object_info['scene']\r\n        )\r\n\r\n        # Filter based on grasp quality and accessibility\r\n        optimal_grasps = self.filter_grasps(grasp_points)\r\n\r\n        return optimal_grasps\n"})}),"\n",(0,t.jsx)(n.h2,{id:"manipulation-planning",children:"Manipulation Planning"}),"\n",(0,t.jsx)(n.h3,{id:"motion-planning-for-humanoid-arms",children:"Motion Planning for Humanoid Arms"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid manipulation requires complex whole-body motion planning:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\nfrom omni.isaac.core.robots import Robot\r\nfrom omni.isaac.core.articulations import ArticulationView\r\n\r\nclass HumanoidManipulationPlanner:\r\n    def __init__(self, robot_name):\r\n        self.robot = Robot(prim_path=f\"/World/{robot_name}\", name=robot_name)\r\n        self.arm_controller = self.setup_arm_controller()\r\n        self.whole_body_planner = self.setup_whole_body_planner()\r\n\r\n    def setup_arm_controller(self):\r\n        # Configure arm-specific controllers\r\n        arm_controller = {\r\n            'left_arm': self.create_arm_controller('left'),\r\n            'right_arm': self.create_arm_controller('right')\r\n        }\r\n        return arm_controller\r\n\r\n    def setup_whole_body_planner(self):\r\n        # Plan for full body coordination during manipulation\r\n        planner = {\r\n            'type': 'whole_body',\r\n            'constraints': self.get_manipulation_constraints(),\r\n            'optimization': self.get_optimization_objectives()\r\n        }\r\n        return planner\r\n\r\n    def plan_manipulation_task(self, target_object, grasp_pose, workspace):\r\n        # Plan whole-body motion for manipulation\r\n        arm_trajectory = self.plan_arm_trajectory(target_object, grasp_pose)\r\n\r\n        # Consider balance constraints for bipedal robot\r\n        base_motion = self.plan_base_motion_for_balance(arm_trajectory)\r\n\r\n        # Coordinate with walking if needed\r\n        footstep_plan = self.plan_footsteps_for_manipulation()\r\n\r\n        return {\r\n            'arm_trajectory': arm_trajectory,\r\n            'base_motion': base_motion,\r\n            'footsteps': footstep_plan\r\n        }\n"})}),"\n",(0,t.jsx)(n.h3,{id:"grasp-planning",children:"Grasp Planning"}),"\n",(0,t.jsx)(n.p,{children:"Plan stable and effective grasps:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class GraspPlanner:\r\n    def __init__(self):\r\n        self.stability_analyzer = self.setup_stability_analyzer()\r\n        self.force_optimizer = self.setup_force_optimizer()\r\n\r\n    def plan_grasp(self, object_info, end_effector_pose):\r\n        # Analyze object properties for grasp planning\r\n        grasp_candidates = self.generate_grasp_candidates(object_info)\r\n\r\n        # Evaluate grasp stability\r\n        stable_grasps = []\r\n        for grasp in grasp_candidates:\r\n            stability_score = self.evaluate_grasp_stability(grasp, object_info)\r\n            if stability_score > self.min_stability_threshold:\r\n                stable_grasps.append((grasp, stability_score))\r\n\r\n        # Select optimal grasp\r\n        optimal_grasp = self.select_optimal_grasp(stable_grasps)\r\n\r\n        return optimal_grasp\r\n\r\n    def evaluate_grasp_stability(self, grasp, object_info):\r\n        # Use physics simulation to evaluate grasp stability\r\n        stability_score = self.stability_analyzer.analyze(\r\n            grasp=grasp,\r\n            object_properties=object_info['properties'],\r\n            contact_points=grasp['contact_points'],\r\n            applied_forces=self.calculate_applied_forces(grasp)\r\n        )\r\n        return stability_score\n"})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-sim-manipulation-tools",children:"Isaac Sim Manipulation Tools"}),"\n",(0,t.jsx)(n.h3,{id:"articulation-view",children:"Articulation View"}),"\n",(0,t.jsx)(n.p,{children:"Control articulated robots efficiently:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from omni.isaac.core.articulations import ArticulationView\r\n\r\nclass IsaacManipulationInterface:\r\n    def __init__(self):\r\n        # Create articulation view for the humanoid robot\r\n        self.humanoid_view = ArticulationView(\r\n            prim_paths_expr=\"/World/Robot/.*\",\r\n            name=\"humanoid_view\"\r\n        )\r\n\r\n        self.humanoid_view.initialize()\r\n\r\n    def execute_manipulation_action(self, joint_positions, cartesian_targets):\r\n        # Execute manipulation using Isaac Sim's physics\r\n        self.humanoid_view.set_joint_positions(joint_positions)\r\n\r\n        # Apply cartesian targets if specified\r\n        if cartesian_targets:\r\n            self.apply_cartesian_targets(cartesian_targets)\r\n\r\n        # Step the physics simulation\r\n        world = self.get_world()\r\n        world.step(render=True)\r\n\r\n    def get_manipulation_feedback(self):\r\n        # Get feedback from manipulation execution\r\n        joint_states = self.humanoid_view.get_joint_positions()\r\n        cartesian_poses = self.humanoid_view.get_end_effectors()\r\n        contact_info = self.get_contact_information()\r\n\r\n        return {\r\n            'joint_states': joint_states,\r\n            'cartesian_poses': cartesian_poses,\r\n            'contacts': contact_info\r\n        }\n"})}),"\n",(0,t.jsx)(n.h3,{id:"contact-and-grasp-simulation",children:"Contact and Grasp Simulation"}),"\n",(0,t.jsx)(n.p,{children:"Realistic contact simulation is crucial for manipulation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ContactSimulator:\r\n    def __init__(self):\r\n        self.contact_sensors = self.setup_contact_sensors()\r\n        self.friction_models = self.setup_friction_models()\r\n\r\n    def setup_contact_sensors(self):\r\n        # Setup contact sensors on fingertips and palm\r\n        contact_sensors = {\r\n            'left_hand': self.create_hand_contact_sensors('left'),\r\n            'right_hand': self.create_hand_contact_sensors('right')\r\n        }\r\n        return contact_sensors\r\n\r\n    def simulate_grasp_interaction(self, grasp_params, object_properties):\r\n        # Simulate the grasp interaction using PhysX\r\n        contact_points = self.calculate_contact_points(grasp_params)\r\n\r\n        # Apply friction and force models\r\n        grasp_success = self.evaluate_grasp_success(\r\n            contact_points=contact_points,\r\n            object_mass=object_properties['mass'],\r\n            applied_forces=grasp_params['applied_forces'],\r\n            friction_coefficients=self.get_friction_coefficients()\r\n        )\r\n\r\n        return {\r\n            'success': grasp_success,\r\n            'contact_forces': self.get_contact_forces(contact_points),\r\n            'slip_probability': self.calculate_slip_probability()\r\n        }\n"})}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-isaac-ros",children:"Integration with Isaac ROS"}),"\n",(0,t.jsx)(n.h3,{id:"ros-bridge-for-manipulation",children:"ROS Bridge for Manipulation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from omni.isaac.ros_bridge import ROSBridge\r\n\r\nclass IsaacROSManipulationBridge:\r\n    def __init__(self):\r\n        self.ros_bridge = ROSBridge()\r\n\r\n        # Create ROS publishers for manipulation\r\n        self.joint_command_pub = self.ros_bridge.create_publisher(\r\n            '/robot/joint_commands',\r\n            'std_msgs/Float64MultiArray'\r\n        )\r\n\r\n        self.grasp_result_pub = self.ros_bridge.create_publisher(\r\n            '/manipulation/grasp_result',\r\n            'std_msgs/Bool'\r\n        )\r\n\r\n        # Create ROS subscribers\r\n        self.manipulation_cmd_sub = self.ros_bridge.create_subscription(\r\n            '/manipulation/command',\r\n            'geometry_msgs/PoseStamped',\r\n            self.manipulation_command_callback\r\n        )\r\n\r\n    def manipulation_command_callback(self, msg):\r\n        # Process manipulation command from ROS\r\n        target_pose = msg.pose\r\n        object_id = msg.header.frame_id\r\n\r\n        # Execute manipulation in Isaac Sim\r\n        result = self.execute_manipulation_in_sim(target_pose, object_id)\r\n\r\n        # Publish result back to ROS\r\n        self.grasp_result_pub.publish(result.success)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"perception-pipeline-integration",children:"Perception Pipeline Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class IsaacROSPerceptionPipeline:\r\n    def __init__(self):\r\n        # Setup Isaac Sim perception nodes\r\n        self.rgb_camera = self.setup_rgb_camera()\r\n        self.depth_camera = self.setup_depth_camera()\r\n        self.lidar = self.setup_lidar()\r\n\r\n        # Connect to ROS topics\r\n        self.setup_ros_interfaces()\r\n\r\n    def setup_rgb_camera(self):\r\n        camera = Camera(\r\n            prim_path=\"/World/Robot/RGB_Camera\",\r\n            frequency=30,\r\n            resolution=(1280, 720)\r\n        )\r\n        return camera\r\n\r\n    def setup_depth_camera(self):\r\n        depth_camera = Camera(\r\n            prim_path=\"/World/Robot/Depth_Camera\",\r\n            frequency=30,\r\n            resolution=(640, 480)\r\n        )\r\n        return depth_camera\r\n\r\n    def process_perception_data(self):\r\n        # Capture data from all sensors\r\n        rgb_data = self.rgb_camera.get_rgb()\r\n        depth_data = self.depth_camera.get_depth()\r\n        lidar_data = self.lidar.get_point_cloud()\r\n\r\n        # Process with Isaac Sim perception tools\r\n        objects = self.detect_objects(rgb_data, depth_data)\r\n        poses = self.estimate_poses(objects, depth_data)\r\n        grasp_points = self.find_grasp_points(objects)\r\n\r\n        # Publish to ROS topics\r\n        self.publish_perception_results(objects, poses, grasp_points)\r\n\r\n        return {\r\n            'objects': objects,\r\n            'poses': poses,\r\n            'grasps': grasp_points\r\n        }\n"})}),"\n",(0,t.jsx)(n.h2,{id:"synthetic-data-for-manipulation-learning",children:"Synthetic Data for Manipulation Learning"}),"\n",(0,t.jsx)(n.h3,{id:"dataset-generation",children:"Dataset Generation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ManipulationDatasetGenerator:\r\n    def __init__(self):\r\n        self.scene_generator = self.setup_scene_generator()\r\n        self.annotation_generator = self.setup_annotation_generator()\r\n\r\n    def generate_manipulation_dataset(self, num_samples=10000):\r\n        dataset = []\r\n\r\n        for i in range(num_samples):\r\n            # Randomize scene\r\n            scene = self.scene_generator.generate_random_scene()\r\n\r\n            # Place manipulable objects\r\n            objects = self.place_manipulable_objects(scene)\r\n\r\n            # Generate manipulation scenarios\r\n            scenarios = self.generate_manipulation_scenarios(objects)\r\n\r\n            # Execute scenarios and collect data\r\n            for scenario in scenarios:\r\n                sample = self.execute_manipulation_scenario(scenario)\r\n                dataset.append(sample)\r\n\r\n            if i % 1000 == 0:\r\n                print(f\"Generated {i}/{num_samples} samples\")\r\n\r\n        return dataset\r\n\r\n    def execute_manipulation_scenario(self, scenario):\r\n        # Execute manipulation scenario in Isaac Sim\r\n        initial_state = scenario['initial_state']\r\n        target = scenario['target_object']\r\n\r\n        # Capture initial perception data\r\n        perception_data = self.capture_perception_data()\r\n\r\n        # Execute manipulation\r\n        success = self.execute_manipulation(target)\r\n\r\n        # Capture final state\r\n        final_state = self.get_robot_state()\r\n\r\n        return {\r\n            'initial_perception': perception_data,\r\n            'manipulation_plan': scenario['plan'],\r\n            'execution_result': success,\r\n            'final_state': final_state\r\n        }\n"})}),"\n",(0,t.jsx)(n.h3,{id:"domain-randomization-for-manipulation",children:"Domain Randomization for Manipulation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ManipulationDomainRandomizer:\r\n    def __init__(self):\r\n        self.object_randomizer = self.setup_object_randomizer()\r\n        self.environment_randomizer = self.setup_environment_randomizer()\r\n        self.lighting_randomizer = self.setup_lighting_randomizer()\r\n\r\n    def randomize_manipulation_scene(self):\r\n        # Randomize objects\r\n        self.object_randomizer.randomize_object_appearances()\r\n        self.object_randomizer.randomize_object_physics()\r\n\r\n        # Randomize environment\r\n        self.environment_randomizer.randomize_surface_materials()\r\n        self.environment_randomizer.randomize_obstacles()\r\n\r\n        # Randomize lighting\r\n        self.lighting_randomizer.randomize_lighting_conditions()\r\n\r\n        # Randomize camera parameters\r\n        self.randomize_camera_noise()\r\n\r\n        return self.get_current_scene_state()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"reinforcement-learning-for-manipulation",children:"Reinforcement Learning for Manipulation"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-sim-rl-environment",children:"Isaac Sim RL Environment"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from omni.isaac.gym import IsaacEnv\r\nfrom omni.isaac.core.utils.torch.maths import torch\r\nfrom omni.isaac.core.articulations import ArticulationView\r\n\r\nclass IsaacManipulationEnv(IsaacEnv):\r\n    def __init__(self, cfg):\r\n        super().__init__(cfg)\r\n\r\n        self.robot = ArticulationView(\r\n            prim_paths_expr="/World/Robot/.*",\r\n            name="robot_view"\r\n        )\r\n\r\n        self.setup_scene()\r\n\r\n    def setup_scene(self):\r\n        # Setup manipulation environment in Isaac Sim\r\n        self.create_objects()\r\n        self.setup_cameras()\r\n        self.setup_sensors()\r\n\r\n    def get_observations(self):\r\n        # Get observation for RL agent\r\n        robot_pos = self.robot.get_world_poses()\r\n        robot_vel = self.robot.get_velocities()\r\n        object_pos = self.get_object_poses()\r\n        camera_data = self.get_camera_observations()\r\n\r\n        obs = torch.cat([robot_pos, robot_vel, object_pos, camera_data])\r\n        return obs\r\n\r\n    def calculate_reward(self, action):\r\n        # Calculate reward for manipulation task\r\n        current_state = self.get_current_state()\r\n        target_achieved = self.check_target_achieved()\r\n\r\n        reward = 0\r\n        if target_achieved:\r\n            reward += 100  # Large reward for success\r\n        else:\r\n            # Small rewards for progress toward target\r\n            distance_to_target = self.calculate_distance_to_target()\r\n            reward -= distance_to_target * 0.1  # Penalty for distance\r\n\r\n        # Penalty for unsafe actions\r\n        if self.check_unsafe_action(action):\r\n            reward -= 10\r\n\r\n        return reward\n'})}),"\n",(0,t.jsx)(n.h2,{id:"safety-and-robustness",children:"Safety and Robustness"}),"\n",(0,t.jsx)(n.h3,{id:"collision-avoidance",children:"Collision Avoidance"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ManipulationSafetyController:\r\n    def __init__(self):\r\n        self.collision_checker = self.setup_collision_checker()\r\n        self.velocity_limiter = self.setup_velocity_limiter()\r\n\r\n    def check_manipulation_safety(self, planned_trajectory):\r\n        # Check if trajectory is collision-free\r\n        collisions = self.collision_checker.check_trajectory(\r\n            trajectory=planned_trajectory,\r\n            robot=self.robot,\r\n            environment=self.get_environment()\r\n        )\r\n\r\n        if collisions:\r\n            # Plan safe alternative trajectory\r\n            safe_trajectory = self.plan_safe_trajectory(\r\n                original_trajectory=planned_trajectory,\r\n                collisions=collisions\r\n            )\r\n            return safe_trajectory\r\n        else:\r\n            return planned_trajectory\r\n\r\n    def enforce_velocity_limits(self, commanded_velocities):\r\n        # Limit velocities for safety\r\n        limited_velocities = torch.clamp(\r\n            commanded_velocities,\r\n            min=-self.max_velocity,\r\n            max=self.max_velocity\r\n        )\r\n        return limited_velocities\n"})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"efficient-simulation",children:"Efficient Simulation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class EfficientManipulationSimulator:\r\n    def __init__(self):\r\n        self.lod_manager = self.setup_lod_manager()\r\n        self.physics_optimizations = self.setup_physics_optimizations()\r\n\r\n    def optimize_manipulation_simulation(self):\r\n        # Use Level of Detail for distant objects\r\n        self.lod_manager.update_lod_levels()\r\n\r\n        # Optimize physics for manipulation tasks\r\n        self.physics_optimizations.enable_sparse_solving()\r\n        self.physics_optimizations.use_fixed_substeps()\r\n\r\n        # Batch multiple manipulation tasks\r\n        self.batch_manipulation_tasks()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"real-world-transfer",children:"Real-World Transfer"}),"\n",(0,t.jsx)(n.h3,{id:"sim-to-real-considerations",children:"Sim-to-Real Considerations"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class SimToRealTransfer:\r\n    def __init__(self):\r\n        self.calibration_tool = self.setup_calibration_tool()\r\n        self.system_identification = self.setup_system_identification()\r\n\r\n    def improve_real_world_performance(self):\r\n        # Calibrate simulation parameters\r\n        self.calibrate_simulation_parameters()\r\n\r\n        # Identify system differences\r\n        self.identify_system_differences()\r\n\r\n        # Adapt control strategies\r\n        self.adapt_control_for_real_world()\r\n\r\n    def validate_manipulation_skills(self, real_robot):\r\n        # Transfer learned manipulation skills to real robot\r\n        skills = self.load_learned_skills()\r\n\r\n        # Test on real robot with safety measures\r\n        success_rate = self.test_skills_on_real_robot(\r\n            skills=skills,\r\n            robot=real_robot,\r\n            safety_limits=self.get_safety_limits()\r\n        )\r\n\r\n        return success_rate\n"})}),"\n",(0,t.jsx)(n.p,{children:"Perception and manipulation in Isaac Sim provide a comprehensive framework for developing sophisticated manipulation capabilities in humanoid robots, with realistic physics simulation, synthetic data generation, and pathways for real-world deployment."})]})}function _(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>o});var a=r(6540);const t={},i=a.createContext(t);function s(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);