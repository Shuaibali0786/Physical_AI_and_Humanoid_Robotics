"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[506],{2089:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module4-vla/voice-to-action","title":"Voice-to-Action with OpenAI Whisper","description":"Introduction to Voice-to-Action Systems","source":"@site/docs/module4-vla/voice-to-action.md","sourceDirName":"module4-vla","slug":"/module4-vla/voice-to-action","permalink":"/docs/module4-vla/voice-to-action","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"modules","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/docs/module4-vla/intro"},"next":{"title":"Cognitive Planning with LLMs to ROS 2 Actions","permalink":"/docs/module4-vla/cognitive-planning"}}');var o=r(4848),t=r(8453);const s={},a="Voice-to-Action with OpenAI Whisper",c={},d=[{value:"Introduction to Voice-to-Action Systems",id:"introduction-to-voice-to-action-systems",level:2},{value:"OpenAI Whisper for Speech Recognition",id:"openai-whisper-for-speech-recognition",level:2},{value:"Overview of Whisper",id:"overview-of-whisper",level:3},{value:"Whisper Architecture",id:"whisper-architecture",level:3},{value:"Whisper Models",id:"whisper-models",level:3},{value:"Implementing Whisper for Robotics",id:"implementing-whisper-for-robotics",level:2},{value:"Basic Whisper Integration",id:"basic-whisper-integration",level:3},{value:"Real-time Audio Processing",id:"real-time-audio-processing",level:3},{value:"Speech Recognition Optimization for Robotics",id:"speech-recognition-optimization-for-robotics",level:2},{value:"Noise Reduction and Filtering",id:"noise-reduction-and-filtering",level:3},{value:"Context-Aware Recognition",id:"context-aware-recognition",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"ROS 2 Node for Voice Processing",id:"ros-2-node-for-voice-processing",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Model Quantization and Optimization",id:"model-quantization-and-optimization",level:3},{value:"Error Handling and Robustness",id:"error-handling-and-robustness",level:2},{value:"Confidence Scoring and Validation",id:"confidence-scoring-and-validation",level:3},{value:"Practical Implementation Considerations",id:"practical-implementation-considerations",level:2},{value:"Latency Optimization",id:"latency-optimization",level:3},{value:"Privacy and Security",id:"privacy-and-security",level:3}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"voice-to-action-with-openai-whisper",children:"Voice-to-Action with OpenAI Whisper"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction-to-voice-to-action-systems",children:"Introduction to Voice-to-Action Systems"}),"\n",(0,o.jsx)(n.p,{children:"Voice-to-action systems enable humanoid robots to understand spoken commands and execute corresponding physical actions. This technology represents a significant advancement in human-robot interaction, allowing users to communicate with robots using natural language rather than specialized interfaces or programming."}),"\n",(0,o.jsx)(n.h2,{id:"openai-whisper-for-speech-recognition",children:"OpenAI Whisper for Speech Recognition"}),"\n",(0,o.jsx)(n.h3,{id:"overview-of-whisper",children:"Overview of Whisper"}),"\n",(0,o.jsx)(n.p,{children:"OpenAI Whisper is a state-of-the-art automatic speech recognition (ASR) system trained on 680,000 hours of multilingual and multitask supervised data. It demonstrates robust performance across various domains and languages, making it ideal for humanoid robotics applications."}),"\n",(0,o.jsx)(n.h3,{id:"whisper-architecture",children:"Whisper Architecture"}),"\n",(0,o.jsx)(n.p,{children:"Whisper uses a multitask decoder architecture:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Encoder"}),": Processes audio input using a 30-second context window"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Decoder"}),": Generates text tokens with language identification"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multilingual capability"}),": Trained on 99 languages"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Performs well with background noise and accents"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"whisper-models",children:"Whisper Models"}),"\n",(0,o.jsx)(n.p,{children:"Different model sizes offer trade-offs between accuracy and computational requirements:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"tiny"}),": 39M parameters, suitable for edge devices"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"base"}),": 74M parameters, good balance of speed and accuracy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"small"}),": 244M parameters, higher accuracy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"medium"}),": 769M parameters, very high accuracy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"large"}),": 1550M parameters, highest accuracy"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"implementing-whisper-for-robotics",children:"Implementing Whisper for Robotics"}),"\n",(0,o.jsx)(n.h3,{id:"basic-whisper-integration",children:"Basic Whisper Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import whisper\r\nimport torch\r\nimport numpy as np\r\nfrom scipy.io import wavfile\r\n\r\nclass VoiceToActionSystem:\r\n    def __init__(self, model_size="medium"):\r\n        # Load Whisper model\r\n        self.model = whisper.load_model(model_size)\r\n\r\n        # Define robot command vocabulary\r\n        self.command_keywords = [\r\n            "move forward", "move backward", "turn left", "turn right",\r\n            "pick up", "put down", "stop", "go", "help", "wait"\r\n        ]\r\n\r\n        # Initialize ROS 2 publisher for robot commands\r\n        self.robot_command_publisher = self.initialize_robot_publisher()\r\n\r\n    def transcribe_audio(self, audio_path):\r\n        # Load and transcribe audio using Whisper\r\n        result = self.model.transcribe(audio_path)\r\n        return result["text"]\r\n\r\n    def process_audio_stream(self, audio_data):\r\n        # Process streaming audio data\r\n        # Convert to appropriate format for Whisper\r\n        audio_np = self.preprocess_audio(audio_data)\r\n\r\n        # Transcribe the audio\r\n        transcription = self.model.transcribe(audio_np)\r\n        return transcription["text"]\r\n\r\n    def preprocess_audio(self, audio_data):\r\n        # Convert audio to format expected by Whisper\r\n        # Whisper expects 16kHz mono audio\r\n        if len(audio_data.shape) > 1:\r\n            # Convert stereo to mono\r\n            audio_data = np.mean(audio_data, axis=1)\r\n\r\n        # Ensure proper sample rate (Whisper expects 16kHz)\r\n        # Resample if necessary\r\n        return audio_data\n'})}),"\n",(0,o.jsx)(n.h3,{id:"real-time-audio-processing",children:"Real-time Audio Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import pyaudio\r\nimport threading\r\nimport queue\r\n\r\nclass RealTimeVoiceProcessor:\r\n    def __init__(self, whisper_system):\r\n        self.whisper_system = whisper_system\r\n        self.audio_queue = queue.Queue()\r\n\r\n        # Audio stream parameters\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n        self.rate = 16000  # Whisper expects 16kHz\r\n        self.chunk = 1024\r\n\r\n        self.audio = pyaudio.PyAudio()\r\n        self.stream = None\r\n        self.recording = False\r\n\r\n    def start_listening(self):\r\n        # Start audio recording in a separate thread\r\n        self.stream = self.audio.open(\r\n            format=self.format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n\r\n        self.recording = True\r\n        recording_thread = threading.Thread(target=self.record_audio)\r\n        recording_thread.start()\r\n\r\n    def record_audio(self):\r\n        # Continuously record audio and add to queue\r\n        while self.recording:\r\n            data = self.stream.read(self.chunk)\r\n            self.audio_queue.put(data)\r\n\r\n    def process_voice_commands(self):\r\n        # Process audio chunks and detect commands\r\n        audio_buffer = []\r\n\r\n        while True:\r\n            try:\r\n                # Get audio chunk from queue\r\n                chunk = self.audio_queue.get(timeout=0.1)\r\n                audio_buffer.append(chunk)\r\n\r\n                # Process every 2 seconds of audio\r\n                if len(audio_buffer) * self.chunk >= self.rate * 2:\r\n                    # Convert buffer to numpy array\r\n                    audio_np = np.frombuffer(b\'\'.join(audio_buffer), dtype=np.int16)\r\n                    audio_np = audio_np.astype(np.float32) / 32768.0  # Normalize\r\n\r\n                    # Transcribe and process\r\n                    text = self.whisper_system.process_audio_stream(audio_np)\r\n                    self.handle_command(text)\r\n\r\n                    # Clear buffer\r\n                    audio_buffer = []\r\n\r\n            except queue.Empty:\r\n                continue\r\n\r\n    def handle_command(self, text):\r\n        # Process transcribed text and execute robot commands\r\n        print(f"Recognized: {text}")\r\n\r\n        # Extract command from text\r\n        command = self.extract_robot_command(text)\r\n\r\n        if command:\r\n            print(f"Executing command: {command}")\r\n            self.execute_robot_command(command)\r\n\r\n    def extract_robot_command(self, text):\r\n        # Simple keyword-based command extraction\r\n        text_lower = text.lower()\r\n\r\n        for keyword in self.whisper_system.command_keywords:\r\n            if keyword in text_lower:\r\n                return keyword\r\n\r\n        return None\n'})}),"\n",(0,o.jsx)(n.h2,{id:"speech-recognition-optimization-for-robotics",children:"Speech Recognition Optimization for Robotics"}),"\n",(0,o.jsx)(n.h3,{id:"noise-reduction-and-filtering",children:"Noise Reduction and Filtering"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import webrtcvad\r\nfrom scipy import signal\r\n\r\nclass RobustSpeechProcessor:\r\n    def __init__(self):\r\n        # Voice Activity Detection\r\n        self.vad = webrtcvad.Vad()\r\n        self.vad.set_mode(1)  # Aggressiveness mode (0-3)\r\n\r\n    def preprocess_for_robot_environment(self, audio_data, sample_rate=16000):\r\n        # Apply noise reduction for robot environments\r\n        # Remove background noise from motors, fans, etc.\r\n\r\n        # Apply high-pass filter to remove low-frequency noise\r\n        b, a = signal.butter(4, 100 / (sample_rate / 2), btype='high')\r\n        filtered_audio = signal.filtfilt(b, a, audio_data)\r\n\r\n        # Apply spectral subtraction for noise reduction\r\n        enhanced_audio = self.spectral_subtraction(filtered_audio)\r\n\r\n        return enhanced_audio\r\n\r\n    def spectral_subtraction(self, audio_data):\r\n        # Simple spectral subtraction noise reduction\r\n        # Calculate noise profile from initial silence\r\n        noise_profile = self.estimate_noise_profile(audio_data[:8000])  # First 0.5s\r\n\r\n        # Apply spectral subtraction\r\n        enhanced = audio_data - noise_profile\r\n        return enhanced\r\n\r\n    def estimate_noise_profile(self, initial_audio):\r\n        # Estimate noise profile from initial audio segment\r\n        # Assumes initial segment contains mostly noise\r\n        return np.mean(initial_audio) * 0.1  # Simplified estimation\n"})}),"\n",(0,o.jsx)(n.h3,{id:"context-aware-recognition",children:"Context-Aware Recognition"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class ContextAwareRecognizer:\r\n    def __init__(self, whisper_system):\r\n        self.whisper_system = whisper_system\r\n        self.current_context = "navigation"  # Default context\r\n\r\n        # Context-specific command weights\r\n        self.context_commands = {\r\n            "navigation": ["move", "turn", "go", "stop", "forward", "backward", "left", "right"],\r\n            "manipulation": ["pick", "put", "grasp", "release", "lift", "place"],\r\n            "interaction": ["hello", "help", "wait", "follow", "come", "stop"]\r\n        }\r\n\r\n    def recognize_with_context(self, audio_data):\r\n        # Transcribe audio with context-aware processing\r\n        transcription = self.whisper_system.process_audio_stream(audio_data)\r\n\r\n        # Apply context-specific command extraction\r\n        command = self.extract_command_with_context(transcription, self.current_context)\r\n        return command\r\n\r\n    def extract_command_with_context(self, text, context):\r\n        # Prioritize commands based on current context\r\n        context_keywords = self.context_commands.get(context, [])\r\n\r\n        # Check context-specific keywords first\r\n        for keyword in context_keywords:\r\n            if keyword.lower() in text.lower():\r\n                return keyword\r\n\r\n        # Fall back to general command recognition\r\n        return self.whisper_system.extract_robot_command(text)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,o.jsx)(n.h3,{id:"ros-2-node-for-voice-processing",children:"ROS 2 Node for Voice Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nfrom sensor_msgs.msg import AudioData\r\n\r\nclass VoiceToActionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('voice_to_action_node')\r\n\r\n        # Initialize Whisper system\r\n        self.whisper_system = VoiceToActionSystem(model_size=\"base\")\r\n\r\n        # Publishers for robot commands\r\n        self.cmd_vel_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\r\n        self.speech_publisher = self.create_publisher(String, '/robot_speech', 10)\r\n\r\n        # Subscriber for audio input\r\n        self.audio_subscription = self.create_subscription(\r\n            AudioData,\r\n            '/audio_input',\r\n            self.audio_callback,\r\n            10\r\n        )\r\n\r\n        # Timer for processing\r\n        self.process_timer = self.create_timer(0.1, self.process_commands)\r\n\r\n        self.pending_commands = []\r\n\r\n    def audio_callback(self, msg):\r\n        # Process incoming audio data\r\n        try:\r\n            # Convert audio message to numpy array\r\n            audio_np = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\r\n\r\n            # Transcribe audio\r\n            text = self.whisper_system.process_audio_stream(audio_np)\r\n\r\n            # Extract and queue command\r\n            command = self.extract_robot_command(text)\r\n            if command:\r\n                self.pending_commands.append(command)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing audio: {e}')\r\n\r\n    def process_commands(self):\r\n        # Process pending voice commands\r\n        while self.pending_commands:\r\n            command = self.pending_commands.pop(0)\r\n            self.execute_robot_command(command)\r\n\r\n    def extract_robot_command(self, text):\r\n        # Extract command from transcribed text\r\n        return self.whisper_system.extract_robot_command(text)\r\n\r\n    def execute_robot_command(self, command):\r\n        # Execute robot command based on voice input\r\n        twist = Twist()\r\n\r\n        if \"forward\" in command or \"go\" in command:\r\n            twist.linear.x = 0.5  # Move forward at 0.5 m/s\r\n        elif \"backward\" in command:\r\n            twist.linear.x = -0.5  # Move backward\r\n        elif \"left\" in command:\r\n            twist.angular.z = 0.5  # Turn left\r\n        elif \"right\" in command:\r\n            twist.angular.z = -0.5  # Turn right\r\n        elif \"stop\" in command:\r\n            # Stop movement (twist is already zero)\r\n            pass\r\n        else:\r\n            self.get_logger().info(f'Unknown command: {command}')\r\n            return\r\n\r\n        # Publish command\r\n        self.cmd_vel_publisher.publish(twist)\r\n        self.get_logger().info(f'Executed command: {command}')\n"})}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"model-quantization-and-optimization",children:"Model Quantization and Optimization"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class OptimizedWhisperSystem:\r\n    def __init__(self, model_size="small"):\r\n        # Load model and apply optimizations\r\n        self.model = whisper.load_model(model_size)\r\n\r\n        # Apply quantization for faster inference\r\n        self.model = self.apply_quantization(self.model)\r\n\r\n        # Set model to evaluation mode\r\n        self.model.eval()\r\n\r\n    def apply_quantization(self, model):\r\n        # Apply dynamic quantization to reduce model size\r\n        import torch.quantization as quantization\r\n\r\n        # Quantize the model\r\n        quantized_model = quantization.quantize_dynamic(\r\n            model, {torch.nn.Linear}, dtype=torch.qint8\r\n        )\r\n\r\n        return quantized_model\r\n\r\n    def optimize_inference(self, audio_data):\r\n        # Optimize inference with torch.jit\r\n        with torch.no_grad():\r\n            result = self.model.transcribe(audio_data)\r\n        return result\n'})}),"\n",(0,o.jsx)(n.h2,{id:"error-handling-and-robustness",children:"Error Handling and Robustness"}),"\n",(0,o.jsx)(n.h3,{id:"confidence-scoring-and-validation",children:"Confidence Scoring and Validation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class RobustVoiceToAction:\r\n    def __init__(self, whisper_system):\r\n        self.whisper_system = whisper_system\r\n        self.confidence_threshold = 0.7  # Minimum confidence for command execution\r\n\r\n    def process_with_confidence(self, audio_data):\r\n        # Get transcription with confidence scoring\r\n        result = self.whisper_system.model.transcribe(\r\n            audio_data,\r\n            return_segments=True\r\n        )\r\n\r\n        # Calculate overall confidence\r\n        confidence = self.calculate_transcription_confidence(result)\r\n\r\n        if confidence >= self.confidence_threshold:\r\n            command = self.extract_robot_command(result["text"])\r\n            return command, confidence\r\n        else:\r\n            return None, confidence\r\n\r\n    def calculate_transcription_confidence(self, result):\r\n        # Calculate confidence based on transcription quality\r\n        # This is a simplified approach - actual implementation would use\r\n        # more sophisticated confidence measures\r\n        text = result["text"]\r\n\r\n        # Check for common error patterns\r\n        error_indicators = ["you know", "um", "uh", "like"]\r\n        error_count = sum(1 for indicator in error_indicators if indicator in text.lower())\r\n\r\n        # Calculate confidence based on text length and error patterns\r\n        confidence = min(1.0, len(text) / 100.0)  # Longer text = higher confidence\r\n        confidence = max(0.0, confidence - error_count * 0.1)  # Penalize error indicators\r\n\r\n        return confidence\r\n\r\n    def handle_uncertain_recognition(self, audio_data):\r\n        # Handle cases where recognition confidence is low\r\n        command, confidence = self.process_with_confidence(audio_data)\r\n\r\n        if confidence < self.confidence_threshold:\r\n            # Request clarification or ignore command\r\n            self.request_clarification()\r\n            return None\r\n        else:\r\n            return command\n'})}),"\n",(0,o.jsx)(n.h2,{id:"practical-implementation-considerations",children:"Practical Implementation Considerations"}),"\n",(0,o.jsx)(n.h3,{id:"latency-optimization",children:"Latency Optimization"}),"\n",(0,o.jsx)(n.p,{children:"For real-time voice-to-action systems in robotics, latency is critical:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio buffering"}),": Use appropriate buffer sizes to balance latency and accuracy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Model selection"}),": Choose model size based on hardware capabilities"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parallel processing"}),": Process audio and execute commands in parallel"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Command queuing"}),": Queue commands for smooth execution"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"privacy-and-security",children:"Privacy and Security"}),"\n",(0,o.jsx)(n.p,{children:"When implementing voice systems in robotics:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Local processing"}),": Process audio locally when possible to protect privacy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Data encryption"}),": Encrypt audio data in transit"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Access controls"}),": Implement proper authentication for voice commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Data retention"}),": Clear audio data after processing"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Voice-to-action systems with OpenAI Whisper enable natural, intuitive interaction with humanoid robots, bridging the gap between human communication and robotic action execution."})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var i=r(6540);const o={},t=i.createContext(o);function s(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);