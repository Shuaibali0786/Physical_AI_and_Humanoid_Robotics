"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[833],{2009:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module4-vla/cognitive-planning","title":"Cognitive Planning with LLMs to ROS 2 Actions","description":"Introduction to Cognitive Planning","source":"@site/docs/module4-vla/cognitive-planning.md","sourceDirName":"module4-vla","slug":"/module4-vla/cognitive-planning","permalink":"/docs/module4-vla/cognitive-planning","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"modules","previous":{"title":"Voice-to-Action with OpenAI Whisper","permalink":"/docs/module4-vla/voice-to-action"},"next":{"title":"Speech Recognition and Natural Language Understanding","permalink":"/docs/module4-vla/speech-recognition"}}');var i=t(4848),o=t(8453);const a={},s="Cognitive Planning with LLMs to ROS 2 Actions",l={},c=[{value:"Introduction to Cognitive Planning",id:"introduction-to-cognitive-planning",level:2},{value:"LLM Integration for Robotics",id:"llm-integration-for-robotics",level:2},{value:"Overview of LLM Capabilities",id:"overview-of-llm-capabilities",level:3},{value:"LLM Selection Criteria",id:"llm-selection-criteria",level:3},{value:"Cognitive Planning Architecture",id:"cognitive-planning-architecture",level:2},{value:"Planning Pipeline",id:"planning-pipeline",level:3},{value:"LLM Client Implementation",id:"llm-client-implementation",level:3},{value:"ROS 2 Action Integration",id:"ros-2-action-integration",level:2},{value:"Action Mapping System",id:"action-mapping-system",level:3},{value:"Context-Aware Planning",id:"context-aware-planning",level:2},{value:"World Model Integration",id:"world-model-integration",level:3},{value:"Safety and Error Handling",id:"safety-and-error-handling",level:2},{value:"Plan Validation and Safety Checks",id:"plan-validation-and-safety-checks",level:3},{value:"Multi-Modal Integration",id:"multi-modal-integration",level:2},{value:"Combining Vision and Language Planning",id:"combining-vision-and-language-planning",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching and Plan Reuse",id:"caching-and-plan-reuse",level:3},{value:"Real-World Deployment Considerations",id:"real-world-deployment-considerations",level:2},{value:"Handling Uncertainty and Ambiguity",id:"handling-uncertainty-and-ambiguity",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"cognitive-planning-with-llms-to-ros-2-actions",children:"Cognitive Planning with LLMs to ROS 2 Actions"})}),"\n",(0,i.jsx)(e.h2,{id:"introduction-to-cognitive-planning",children:"Introduction to Cognitive Planning"}),"\n",(0,i.jsx)(e.p,{children:"Cognitive planning in robotics involves high-level reasoning that bridges natural language commands with low-level robot actions. Large Language Models (LLMs) have revolutionized this field by enabling robots to understand complex, multi-step instructions and generate appropriate sequences of ROS 2 actions to achieve user goals."}),"\n",(0,i.jsx)(e.h2,{id:"llm-integration-for-robotics",children:"LLM Integration for Robotics"}),"\n",(0,i.jsx)(e.h3,{id:"overview-of-llm-capabilities",children:"Overview of LLM Capabilities"}),"\n",(0,i.jsx)(e.p,{children:"Large Language Models like GPT-4, Claude, and open-source alternatives provide several key capabilities for robotic planning:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Natural language understanding"}),": Parse complex, ambiguous, or context-dependent commands"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Reasoning and inference"}),": Plan multi-step sequences and handle contingencies"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Knowledge integration"}),": Access world knowledge to inform planning decisions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Adaptability"}),": Handle novel situations and commands not explicitly programmed"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"llm-selection-criteria",children:"LLM Selection Criteria"}),"\n",(0,i.jsx)(e.p,{children:"When choosing an LLM for robotic cognitive planning:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Response time"}),": Critical for real-time interaction"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cost considerations"}),": Balance performance with operational costs"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Reliability"}),": Consistent performance for safety-critical applications"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Customization"}),": Ability to fine-tune for specific robotic tasks"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Privacy"}),": Local models vs. cloud-based services"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"cognitive-planning-architecture",children:"Cognitive Planning Architecture"}),"\n",(0,i.jsx)(e.h3,{id:"planning-pipeline",children:"Planning Pipeline"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import json\r\nimport re\r\nfrom typing import List, Dict, Any\r\nfrom dataclasses import dataclass\r\n\r\n@dataclass\r\nclass RobotAction:\r\n    """Represents a single robot action"""\r\n    action_type: str\r\n    parameters: Dict[str, Any]\r\n    description: str\r\n\r\nclass CognitivePlanner:\r\n    def __init__(self, llm_client, robot_capabilities):\r\n        self.llm_client = llm_client\r\n        self.robot_capabilities = robot_capabilities\r\n        self.action_history = []\r\n\r\n    def plan_from_command(self, user_command: str) -> List[RobotAction]:\r\n        """Generate action sequence from natural language command"""\r\n        # 1. Parse the command using LLM\r\n        parsed_intent = self.parse_command_intent(user_command)\r\n\r\n        # 2. Generate plan using LLM and robot knowledge\r\n        action_sequence = self.generate_action_plan(parsed_intent, user_command)\r\n\r\n        # 3. Validate plan against robot capabilities\r\n        validated_plan = self.validate_plan(action_sequence)\r\n\r\n        return validated_plan\r\n\r\n    def parse_command_intent(self, command: str) -> Dict[str, Any]:\r\n        """Parse user command to extract intent and parameters"""\r\n        prompt = f"""\r\n        Analyze the following robot command and extract the intent and parameters:\r\n        Command: "{command}"\r\n\r\n        Return a JSON object with:\r\n        - "action": Primary action to perform\r\n        - "target_object": Object to interact with (if any)\r\n        - "location": Target location (if any)\r\n        - "parameters": Additional parameters\r\n\r\n        Example:\r\n        {{\r\n            "action": "navigate_to",\r\n            "target_object": "red cup",\r\n            "location": "kitchen table",\r\n            "parameters": {{"speed": "medium"}}\r\n        }}\r\n        """\r\n\r\n        response = self.llm_client.generate(prompt)\r\n        return json.loads(response)\r\n\r\n    def generate_action_plan(self, intent: Dict[str, Any], original_command: str) -> List[RobotAction]:\r\n        """Generate detailed action sequence from parsed intent"""\r\n        prompt = f"""\r\n        Given the following user intent and robot capabilities, generate a detailed action sequence:\r\n\r\n        User Intent: {json.dumps(intent)}\r\n        Robot Capabilities: {json.dumps(self.robot_capabilities)}\r\n\r\n        Generate a sequence of specific robot actions that will accomplish the user\'s goal.\r\n        Each action should be executable by the robot\'s ROS 2 interface.\r\n\r\n        Return as a list of JSON objects with:\r\n        - "action_type": Type of action (e.g., "navigate_to", "pick_object", "place_object")\r\n        - "parameters": Parameters for the action\r\n        - "description": Human-readable description\r\n\r\n        Example:\r\n        [\r\n            {{\r\n                "action_type": "navigate_to",\r\n                "parameters": {{"location": "kitchen"}},\r\n                "description": "Move to the kitchen area"\r\n            }},\r\n            {{\r\n                "action_type": "find_object",\r\n                "parameters": {{"object": "red cup"}},\r\n                "description": "Locate the red cup"\r\n            }}\r\n        ]\r\n        """\r\n\r\n        response = self.llm_client.generate(prompt)\r\n        action_list = json.loads(response)\r\n\r\n        # Convert to RobotAction objects\r\n        actions = []\r\n        for action_dict in action_list:\r\n            action = RobotAction(\r\n                action_type=action_dict["action_type"],\r\n                parameters=action_dict["parameters"],\r\n                description=action_dict["description"]\r\n            )\r\n            actions.append(action)\r\n\r\n        return actions\r\n\r\n    def validate_plan(self, action_sequence: List[RobotAction]) -> List[RobotAction]:\r\n        """Validate action sequence against robot capabilities"""\r\n        validated_actions = []\r\n\r\n        for action in action_sequence:\r\n            if self.is_action_valid(action):\r\n                validated_actions.append(action)\r\n            else:\r\n                # Handle invalid action\r\n                corrected_action = self.handle_invalid_action(action)\r\n                if corrected_action:\r\n                    validated_actions.append(corrected_action)\r\n\r\n        return validated_actions\r\n\r\n    def is_action_valid(self, action: RobotAction) -> bool:\r\n        """Check if action is supported by robot"""\r\n        return action.action_type in self.robot_capabilities\r\n\r\n    def handle_invalid_action(self, action: RobotAction) -> RobotAction:\r\n        """Handle invalid actions by suggesting alternatives"""\r\n        # This could involve LLM-based alternative generation\r\n        # or fallback to safe behaviors\r\n        print(f"Warning: Invalid action \'{action.action_type}\'")\r\n        return None\n'})}),"\n",(0,i.jsx)(e.h3,{id:"llm-client-implementation",children:"LLM Client Implementation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import openai\r\nimport time\r\nfrom typing import Optional\r\n\r\nclass LLMClient:\r\n    def __init__(self, api_key: str, model: str = "gpt-4"):\r\n        self.api_key = api_key\r\n        self.model = model\r\n        openai.api_key = api_key\r\n\r\n    def generate(self, prompt: str, max_retries: int = 3) -> str:\r\n        """Generate response from LLM with error handling"""\r\n        for attempt in range(max_retries):\r\n            try:\r\n                response = openai.ChatCompletion.create(\r\n                    model=self.model,\r\n                    messages=[{"role": "user", "content": prompt}],\r\n                    temperature=0.1,  # Low temperature for consistency\r\n                    max_tokens=1000\r\n                )\r\n                return response.choices[0].message.content.strip()\r\n\r\n            except openai.error.RateLimitError:\r\n                print(f"Rate limit reached, waiting... (attempt {attempt + 1})")\r\n                time.sleep(2 ** attempt)  # Exponential backoff\r\n            except openai.error.APIError as e:\r\n                print(f"API Error: {e}")\r\n                time.sleep(2 ** attempt)\r\n            except Exception as e:\r\n                print(f"Unexpected error: {e}")\r\n                break\r\n\r\n        raise Exception(f"Failed to get response after {max_retries} attempts")\r\n\r\n    def generate_with_validation(self, prompt: str, expected_format: str) -> Optional[Dict]:\r\n        """Generate response and validate format"""\r\n        response = self.generate(prompt)\r\n\r\n        # Validate JSON format\r\n        try:\r\n            result = json.loads(response)\r\n            if self.validate_format(result, expected_format):\r\n                return result\r\n        except json.JSONDecodeError:\r\n            pass\r\n\r\n        # If format is invalid, request correction\r\n        correction_prompt = f"""\r\n        The previous response was not in the expected JSON format.\r\n        Expected format: {expected_format}\r\n        Response: {response}\r\n\r\n        Please return the response in the correct JSON format.\r\n        """\r\n        corrected_response = self.generate(correction_prompt)\r\n        return json.loads(corrected_response)\r\n\r\n    def validate_format(self, result: Dict, expected_format: str) -> bool:\r\n        """Validate response format"""\r\n        # Implement format validation logic\r\n        # This is a simplified example\r\n        return isinstance(result, dict)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"ros-2-action-integration",children:"ROS 2 Action Integration"}),"\n",(0,i.jsx)(e.h3,{id:"action-mapping-system",children:"Action Mapping System"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.action import ActionClient\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import Pose, Point\r\nfrom move_base_msgs.msg import MoveBaseGoal\r\nfrom std_msgs.msg import String\r\n\r\nclass ROS2ActionExecutor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'cognitive_planner_executor\')\r\n\r\n        # ROS 2 action clients\r\n        self.move_base_client = ActionClient(self, MoveBaseGoal, \'move_base\')\r\n        self.manipulation_client = ActionClient(self, \'ManipulationGoal\', \'manipulation\')\r\n\r\n        # Publishers\r\n        self.speech_publisher = self.create_publisher(String, \'robot_speech\', 10)\r\n\r\n    def execute_action(self, action: RobotAction) -> bool:\r\n        """Execute a single robot action"""\r\n        try:\r\n            if action.action_type == "navigate_to":\r\n                return self.execute_navigation_action(action)\r\n            elif action.action_type == "pick_object":\r\n                return self.execute_manipulation_action(action, "pick")\r\n            elif action.action_type == "place_object":\r\n                return self.execute_manipulation_action(action, "place")\r\n            elif action.action_type == "find_object":\r\n                return self.execute_perception_action(action)\r\n            else:\r\n                self.get_logger().warn(f"Unknown action type: {action.action_type}")\r\n                return False\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error executing action {action.action_type}: {e}")\r\n            return False\r\n\r\n    def execute_navigation_action(self, action: RobotAction) -> bool:\r\n        """Execute navigation action"""\r\n        location = action.parameters.get("location", "default")\r\n        target_pose = self.get_pose_for_location(location)\r\n\r\n        if not target_pose:\r\n            self.get_logger().error(f"Unknown location: {location}")\r\n            return False\r\n\r\n        # Send navigation goal\r\n        goal_msg = MoveBaseGoal()\r\n        goal_msg.target_pose.header.frame_id = "map"\r\n        goal_msg.target_pose.header.stamp = self.get_clock().now().to_msg()\r\n        goal_msg.target_pose.pose = target_pose\r\n\r\n        self.move_base_client.wait_for_server()\r\n        future = self.move_base_client.send_goal(goal_msg)\r\n        rclpy.spin_until_future_complete(self, future)\r\n\r\n        return future.result().success\r\n\r\n    def execute_manipulation_action(self, action: RobotAction, manipulation_type: str) -> bool:\r\n        """Execute manipulation action"""\r\n        # Implementation depends on specific manipulation capabilities\r\n        # This is a simplified example\r\n        object_name = action.parameters.get("object", "")\r\n\r\n        goal_msg = {\r\n            "type": manipulation_type,\r\n            "object": object_name,\r\n            "location": action.parameters.get("location", "default")\r\n        }\r\n\r\n        # Send manipulation goal\r\n        self.manipulation_client.wait_for_server()\r\n        future = self.manipulation_client.send_goal(goal_msg)\r\n        rclpy.spin_until_future_complete(self, future)\r\n\r\n        return future.result().success\r\n\r\n    def get_pose_for_location(self, location_name: str) -> Optional[Pose]:\r\n        """Get predefined pose for location name"""\r\n        # This would typically come from a map or database\r\n        location_poses = {\r\n            "kitchen": Pose(position=Point(x=1.0, y=2.0, z=0.0)),\r\n            "living_room": Pose(position=Point(x=3.0, y=1.0, z=0.0)),\r\n            "bedroom": Pose(position=Point(x=0.0, y=5.0, z=0.0))\r\n        }\r\n\r\n        return location_poses.get(location_name)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"context-aware-planning",children:"Context-Aware Planning"}),"\n",(0,i.jsx)(e.h3,{id:"world-model-integration",children:"World Model Integration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class WorldModel:\r\n    def __init__(self):\r\n        self.objects = {}\r\n        self.locations = {}\r\n        self.robot_state = {}\r\n        self.update_timestamp = time.time()\r\n\r\n    def update_object_location(self, object_id: str, location: str):\r\n        """Update object location in world model"""\r\n        if object_id not in self.objects:\r\n            self.objects[object_id] = {}\r\n        self.objects[object_id]["location"] = location\r\n        self.objects[object_id]["last_seen"] = time.time()\r\n\r\n    def get_object_location(self, object_id: str) -> Optional[str]:\r\n        """Get current location of object"""\r\n        obj = self.objects.get(object_id)\r\n        if obj and time.time() - obj.get("last_seen", 0) < 300:  # 5 minutes\r\n            return obj.get("location")\r\n        return None\r\n\r\n    def update_robot_location(self, location: str):\r\n        """Update robot location in world model"""\r\n        self.robot_state["location"] = location\r\n        self.robot_state["last_updated"] = time.time()\r\n\r\n    def get_robot_location(self) -> Optional[str]:\r\n        """Get current robot location"""\r\n        return self.robot_state.get("location")\r\n\r\nclass ContextAwarePlanner(CognitivePlanner):\r\n    def __init__(self, llm_client, robot_capabilities, world_model):\r\n        super().__init__(llm_client, robot_capabilities)\r\n        self.world_model = world_model\r\n\r\n    def generate_action_plan(self, intent: Dict[str, Any], original_command: str) -> List[RobotAction]:\r\n        """Generate action plan with context awareness"""\r\n        # Get current world state\r\n        current_state = self.get_current_world_state()\r\n\r\n        # Include context in planning prompt\r\n        prompt = f"""\r\n        Given the following user intent, robot capabilities, and current world state,\r\n        generate a detailed action sequence:\r\n\r\n        User Intent: {json.dumps(intent)}\r\n        Robot Capabilities: {json.dumps(self.robot_capabilities)}\r\n        Current World State: {json.dumps(current_state)}\r\n\r\n        Consider the current state when planning:\r\n        - Object locations may have changed\r\n        - Robot may already be in a favorable position\r\n        - Recent actions may affect the plan\r\n\r\n        Generate a sequence of specific robot actions that will accomplish the user\'s goal.\r\n        Return as a list of JSON objects with "action_type", "parameters", and "description".\r\n        """\r\n\r\n        response = self.llm_client.generate(prompt)\r\n        action_list = json.loads(response)\r\n\r\n        # Convert to RobotAction objects\r\n        actions = []\r\n        for action_dict in action_list:\r\n            action = RobotAction(\r\n                action_type=action_dict["action_type"],\r\n                parameters=action_dict["parameters"],\r\n                description=action_dict["description"]\r\n            )\r\n            actions.append(action)\r\n\r\n        return actions\r\n\r\n    def get_current_world_state(self) -> Dict[str, Any]:\r\n        """Get current state of world model"""\r\n        return {\r\n            "objects": self.world_model.objects,\r\n            "robot_location": self.world_model.get_robot_location(),\r\n            "last_update": self.world_model.update_timestamp\r\n        }\n'})}),"\n",(0,i.jsx)(e.h2,{id:"safety-and-error-handling",children:"Safety and Error Handling"}),"\n",(0,i.jsx)(e.h3,{id:"plan-validation-and-safety-checks",children:"Plan Validation and Safety Checks"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class SafeCognitivePlanner(ContextAwarePlanner):\r\n    def __init__(self, llm_client, robot_capabilities, world_model):\r\n        super().__init__(llm_client, robot_capabilities, world_model)\r\n        self.safety_constraints = self.define_safety_constraints()\r\n\r\n    def define_safety_constraints(self):\r\n        """Define safety constraints for planning"""\r\n        return {\r\n            "no_go_zones": ["staircase", "construction_area"],\r\n            "speed_limits": {"navigation": 0.5, "manipulation": 0.1},\r\n            "force_limits": {"gripping": 50.0},  # Newtons\r\n            "timeouts": {"navigation": 60.0, "manipulation": 120.0}\r\n        }\r\n\r\n    def validate_plan(self, action_sequence: List[RobotAction]) -> List[RobotAction]:\r\n        """Validate action sequence with safety checks"""\r\n        validated_actions = []\r\n\r\n        for action in action_sequence:\r\n            # Check safety constraints\r\n            if self.is_action_safe(action):\r\n                validated_actions.append(action)\r\n            else:\r\n                # Handle unsafe action\r\n                safe_alternative = self.generate_safe_alternative(action)\r\n                if safe_alternative:\r\n                    validated_actions.append(safe_alternative)\r\n                else:\r\n                    # Skip unsafe action and continue\r\n                    continue\r\n\r\n        return validated_actions\r\n\r\n    def is_action_safe(self, action: RobotAction) -> bool:\r\n        """Check if action is safe to execute"""\r\n        if action.action_type == "navigate_to":\r\n            location = action.parameters.get("location", "")\r\n            if location in self.safety_constraints["no_go_zones"]:\r\n                return False\r\n\r\n        elif action.action_type == "manipulate_object":\r\n            force = action.parameters.get("force", 0.0)\r\n            max_force = self.safety_constraints["force_limits"]["gripping"]\r\n            if force > max_force:\r\n                return False\r\n\r\n        return True\r\n\r\n    def generate_safe_alternative(self, action: RobotAction) -> Optional[RobotAction]:\r\n        """Generate safe alternative to unsafe action"""\r\n        if action.action_type == "navigate_to":\r\n            # Suggest safe alternative route\r\n            original_location = action.parameters.get("location", "")\r\n            safe_location = self.find_safe_alternative_location(original_location)\r\n            if safe_location:\r\n                return RobotAction(\r\n                    action_type="navigate_to",\r\n                    parameters={"location": safe_location},\r\n                    description=f"Navigate to safe alternative location: {safe_location}"\r\n                )\r\n\r\n        elif action.action_type == "manipulate_object":\r\n            # Reduce force to safe level\r\n            safe_force = self.safety_constraints["force_limits"]["gripping"]\r\n            action.parameters["force"] = safe_force\r\n            action.description += " (force limited for safety)"\r\n\r\n        return action\r\n\r\n    def find_safe_alternative_location(self, original_location: str) -> Optional[str]:\r\n        """Find safe alternative to dangerous location"""\r\n        # Implementation would find nearby safe locations\r\n        # This is a simplified example\r\n        safe_alternatives = {\r\n            "staircase": "nearby hallway",\r\n            "construction_area": "adjacent room"\r\n        }\r\n        return safe_alternatives.get(original_location)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"multi-modal-integration",children:"Multi-Modal Integration"}),"\n",(0,i.jsx)(e.h3,{id:"combining-vision-and-language-planning",children:"Combining Vision and Language Planning"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class MultiModalCognitivePlanner(SafeCognitivePlanner):\r\n    def __init__(self, llm_client, robot_capabilities, world_model, vision_system):\r\n        super().__init__(llm_client, robot_capabilities, world_model)\r\n        self.vision_system = vision_system\r\n\r\n    def generate_action_plan(self, intent: Dict[str, Any], original_command: str) -> List[RobotAction]:\r\n        """Generate action plan with multi-modal input"""\r\n        # Get visual information\r\n        visual_info = self.vision_system.get_current_scene_description()\r\n\r\n        # Include visual context in planning\r\n        prompt = f"""\r\n        Given the following user intent, robot capabilities, current world state, and visual scene,\r\n        generate a detailed action sequence:\r\n\r\n        User Intent: {json.dumps(intent)}\r\n        Robot Capabilities: {json.dumps(self.robot_capabilities)}\r\n        Current World State: {json.dumps(self.get_current_world_state())}\r\n        Visual Scene: {json.dumps(visual_info)}\r\n\r\n        Use visual information to refine the plan:\r\n        - Confirm object existence and location\r\n        - Identify potential obstacles\r\n        - Adapt plan based on actual scene\r\n\r\n        Generate a sequence of specific robot actions that will accomplish the user\'s goal.\r\n        Return as a list of JSON objects with "action_type", "parameters", and "description".\r\n        """\r\n\r\n        response = self.llm_client.generate(prompt)\r\n        action_list = json.loads(response)\r\n\r\n        # Convert to RobotAction objects\r\n        actions = []\r\n        for action_dict in action_list:\r\n            action = RobotAction(\r\n                action_type=action_dict["action_type"],\r\n                parameters=action_dict["parameters"],\r\n                description=action_dict["description"]\r\n            )\r\n            actions.append(action)\r\n\r\n        return actions\r\n\r\n    def validate_with_vision(self, action: RobotAction) -> bool:\r\n        """Validate action using visual feedback"""\r\n        if action.action_type == "navigate_to":\r\n            target_location = action.parameters.get("location", "")\r\n            # Check if path is clear using vision\r\n            obstacles = self.vision_system.detect_obstacles_to_location(target_location)\r\n            return len(obstacles) == 0\r\n\r\n        elif action.action_type == "pick_object":\r\n            target_object = action.parameters.get("object", "")\r\n            # Check if object is visible and reachable\r\n            object_info = self.vision_system.get_object_info(target_object)\r\n            return object_info is not None and object_info["reachable"]\r\n\r\n        return True\n'})}),"\n",(0,i.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(e.h3,{id:"caching-and-plan-reuse",children:"Caching and Plan Reuse"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import hashlib\r\nfrom datetime import datetime, timedelta\r\n\r\nclass OptimizedCognitivePlanner(MultiModalCognitivePlanner):\r\n    def __init__(self, llm_client, robot_capabilities, world_model, vision_system):\r\n        super().__init__(llm_client, robot_capabilities, world_model, vision_system)\r\n        self.plan_cache = {}\r\n        self.cache_ttl = timedelta(minutes=30)\r\n\r\n    def plan_from_command(self, user_command: str) -> List[RobotAction]:\r\n        """Generate plan with caching"""\r\n        # Create cache key\r\n        cache_key = hashlib.md5(user_command.encode()).hexdigest()\r\n\r\n        # Check cache\r\n        cached_result = self.get_cached_plan(cache_key, user_command)\r\n        if cached_result:\r\n            print("Using cached plan")\r\n            return cached_result\r\n\r\n        # Generate new plan\r\n        plan = super().plan_from_command(user_command)\r\n\r\n        # Cache the plan\r\n        self.cache_plan(cache_key, user_command, plan)\r\n\r\n        return plan\r\n\r\n    def get_cached_plan(self, cache_key: str, command: str) -> Optional[List[RobotAction]]:\r\n        """Get cached plan if still valid"""\r\n        cached = self.plan_cache.get(cache_key)\r\n        if cached:\r\n            plan, timestamp, original_command = cached\r\n            if datetime.now() - timestamp < self.cache_ttl:\r\n                # Verify plan is still valid given current state\r\n                if self.is_plan_still_valid(plan, original_command):\r\n                    return plan\r\n\r\n        return None\r\n\r\n    def cache_plan(self, cache_key: str, command: str, plan: List[RobotAction]):\r\n        """Cache generated plan"""\r\n        self.plan_cache[cache_key] = (plan, datetime.now(), command)\r\n\r\n    def is_plan_still_valid(self, plan: List[RobotAction], original_command: str) -> bool:\r\n        """Check if cached plan is still valid"""\r\n        # Check if world state has changed significantly\r\n        current_state = self.get_current_world_state()\r\n        # Implementation would compare current state with state when plan was made\r\n        return True  # Simplified for example\n'})}),"\n",(0,i.jsx)(e.h2,{id:"real-world-deployment-considerations",children:"Real-World Deployment Considerations"}),"\n",(0,i.jsx)(e.h3,{id:"handling-uncertainty-and-ambiguity",children:"Handling Uncertainty and Ambiguity"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class RobustCognitivePlanner(OptimizedCognitivePlanner):\r\n    def __init__(self, llm_client, robot_capabilities, world_model, vision_system):\r\n        super().__init__(llm_client, robot_capabilities, world_model, vision_system)\r\n        self.confidence_threshold = 0.7\r\n\r\n    def handle_ambiguous_command(self, command: str) -> List[RobotAction]:\r\n        """Handle ambiguous or unclear commands"""\r\n        # Ask clarifying questions using LLM\r\n        clarification_prompt = f"""\r\n        The following command is ambiguous. Generate 2-3 clarifying questions\r\n        that would help understand what the user wants:\r\n\r\n        Command: "{command}"\r\n\r\n        Return as a JSON list of questions.\r\n        """\r\n\r\n        questions = self.llm_client.generate(clarification_prompt)\r\n        questions_list = json.loads(questions)\r\n\r\n        # Ask user for clarification\r\n        for question in questions_list:\r\n            response = self.ask_user_for_clarification(question)\r\n            if response:\r\n                # Regenerate plan with clarification\r\n                clarified_command = f"{command} - {question} - {response}"\r\n                return self.plan_from_command(clarified_command)\r\n\r\n        # If no clarification received, use default interpretation\r\n        return self.plan_from_command(command)\r\n\r\n    def ask_user_for_clarification(self, question: str) -> Optional[str]:\r\n        """Ask user for clarification (implementation depends on interaction method)"""\r\n        # This would typically involve speech output or GUI\r\n        print(f"Robot: {question}")\r\n        # In practice, this would wait for user response\r\n        return None  # Simplified for example\r\n\r\n    def execute_with_monitoring(self, action_sequence: List[RobotAction]) -> bool:\r\n        """Execute action sequence with monitoring and error recovery"""\r\n        for i, action in enumerate(action_sequence):\r\n            print(f"Executing action {i+1}/{len(action_sequence)}: {action.description}")\r\n\r\n            # Execute action\r\n            success = self.execute_action(action)\r\n\r\n            if not success:\r\n                # Handle failure\r\n                recovery_plan = self.generate_recovery_plan(action, action_sequence[i+1:])\r\n                if recovery_plan:\r\n                    success = self.execute_with_monitoring(recovery_plan)\r\n                    if success:\r\n                        continue\r\n\r\n            if not success:\r\n                print(f"Action failed and no recovery possible: {action.description}")\r\n                return False\r\n\r\n        return True\r\n\r\n    def generate_recovery_plan(self, failed_action: RobotAction, remaining_actions: List[RobotAction]) -> List[RobotAction]:\r\n        """Generate recovery plan when action fails"""\r\n        recovery_prompt = f"""\r\n        The following action failed:\r\n        Action: {failed_action.action_type}\r\n        Parameters: {json.dumps(failed_action.parameters)}\r\n        Description: {failed_action.description}\r\n\r\n        Remaining actions: {json.dumps([a.description for a in remaining_actions])}\r\n\r\n        Generate a recovery plan that addresses the failure and continues toward the goal.\r\n        Return as a list of JSON objects with "action_type", "parameters", and "description".\r\n        """\r\n\r\n        try:\r\n            recovery_response = self.llm_client.generate(recovery_prompt)\r\n            recovery_actions = json.loads(recovery_response)\r\n\r\n            # Convert to RobotAction objects\r\n            robot_actions = []\r\n            for action_dict in recovery_actions:\r\n                robot_action = RobotAction(\r\n                    action_type=action_dict["action_type"],\r\n                    parameters=action_dict["parameters"],\r\n                    description=action_dict["description"]\r\n                )\r\n                robot_actions.append(robot_action)\r\n\r\n            return robot_actions\r\n        except:\r\n            return []  # No recovery plan available\n'})}),"\n",(0,i.jsx)(e.p,{children:"Cognitive planning with LLMs bridges the gap between natural language commands and executable robot actions, enabling more intuitive and flexible human-robot interaction in complex tasks."})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>s});var r=t(6540);const i={},o=r.createContext(i);function a(n){const e=r.useContext(o);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:a(n.components),r.createElement(o.Provider,{value:e},n.children)}}}]);